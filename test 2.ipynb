{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c6bbd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📬 5 messages since 27-Apr-2025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] 恭喜，手机号码邮箱已开通\n",
      "    From: 网易邮件中心 <mail@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ↪ 0 rows parsed from 正文\n",
      "\n",
      "[2] 邮件办公，如此轻松！欢迎来到网易邮箱！\n",
      "    From: 网易邮箱助手 <club@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ↪ 0 rows parsed from 正文\n",
      "\n",
      "[3] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['Book2.xlsx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ↪ 0 rows parsed from Book2.xlsx\n",
      "\n",
      "[4] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (2): ['Book2.xlsx', 'SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls']\n",
      "    ↪ 0 rows parsed from Book2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ↪ GLM parsed 8 row(s) from SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls\n",
      "\n",
      "[5] Sssa test2基金净值净值\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|██████████| 5/5 [00:38<00:00,  7.74s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ↪ GLM parsed 4 row(s) from SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls\n",
      "\n",
      "✅ 12 rows written → 2025-05-27 基金净值.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fund-NAV harvester v0.8\n",
    "───────────────────────────────────────────────────────────────────────────\n",
    "1. IMAP login (163.com, ID handshake)\n",
    "2. For each message:\n",
    "      • capture subject + sender + full body text\n",
    "      • capture every attachment (any filename)\n",
    "      • send ⟨subject + body + attachment text⟩ to GLM-Z1-Flash\n",
    "3. Parse alphanumeric fund codes & write rows →  YYYY-MM-DD 基金净值.xlsx\n",
    "\"\"\"\n",
    "\n",
    "import re, json, tempfile, pathlib, datetime, contextlib, io, warnings\n",
    "from imapclient import IMAPClient\n",
    "import pyzmail, pandas as pd, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional, nicer HTML-to-text if bs4 is around\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    def html2text(html:str)->str:\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\")\n",
    "except ImportError:\n",
    "    def html2text(html:str)->str:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", html)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# ─── creds & endpoints ──────────────────────────────────────────────────\n",
    "IMAP_HOST  = \"imap.163.com\"\n",
    "EMAIL_USER = \"zhanluekehu@163.com\"\n",
    "EMAIL_PWD  = \"DRqdN38whrnCFPGx\"              # 163 授权码\n",
    "GLM_KEY    = \"afe7583d73c9d3948f60230e79e08151.Z9HPB84mxuC31DeK\"\n",
    "GLM_URL    = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "MODEL      = \"glm-z1-flash\"\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "TODAY   = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "XLSX    = f\"{TODAY} 基金净值.xlsx\"\n",
    "SHEET   = TODAY\n",
    "COLS    = [\"日期\",\"基金名称\",\"基金代码\",\"单位净值\",\"累计净值\",\n",
    "           \"原邮件名\",\"发件人\",\"发件机构\"]\n",
    "\n",
    "# allow 2-10 char alphanumeric fund codes\n",
    "RX = re.compile(\n",
    "    r\"(\\d{4}-\\d{2}-\\d{2}).*?\"          # 日期\n",
    "    r\"([\\u4e00-\\u9fa5\\w]+).*?\"         # 基金名称\n",
    "    r\"([A-Za-z0-9]{2,10}).*?\"          # 基金代码\n",
    "    r\"([\\d.]+).*?\"                     # 单位净值\n",
    "    r\"([\\d.]+)\",                       # 累计净值\n",
    "    re.S)\n",
    "\n",
    "# ─── helper: fetch recent mail ──────────────────────────────────────────\n",
    "def fetch_mail(days:int=30):\n",
    "    with IMAPClient(IMAP_HOST, ssl=True) as srv:\n",
    "        srv.login(EMAIL_USER, EMAIL_PWD)\n",
    "        try:\n",
    "            srv.id_({\"name\":\"python\",\"version\":\"0.8\",\"vendor\":\"myclient\",\n",
    "                     \"contact\":EMAIL_USER})\n",
    "        except Exception:\n",
    "            pass\n",
    "        srv.select_folder(\"INBOX\")\n",
    "        since = (datetime.date.today()-datetime.timedelta(days=days)\n",
    "                ).strftime(\"%d-%b-%Y\")\n",
    "        ids = srv.search([\"SINCE\", since])\n",
    "        print(f\"📬 {len(ids)} messages since {since}\\n\")\n",
    "        for mid in tqdm(ids, desc=\"Fetching\", unit=\"mail\"):\n",
    "            raw = srv.fetch([mid], [\"RFC822\"])[mid][b\"RFC822\"]\n",
    "            yield pyzmail.PyzMessage.factory(raw)\n",
    "\n",
    "# ─── helper: full body text ─────────────────────────────────────────────\n",
    "def get_body(msg):\n",
    "    if msg.text_part:\n",
    "        charset = msg.text_part.charset or \"utf-8\"\n",
    "        return msg.text_part.get_payload().decode(charset, \"ignore\")\n",
    "    if msg.html_part:\n",
    "        charset = msg.html_part.charset or \"utf-8\"\n",
    "        html = msg.html_part.get_payload().decode(charset, \"ignore\")\n",
    "        return html2text(html)\n",
    "    return \"\"\n",
    "\n",
    "# ─── helper: list attachments (filename, bytes) ─────────────────────────\n",
    "def list_attachments(msg):\n",
    "    for part in msg.mailparts:\n",
    "        fn = getattr(part, \"filename\", None)\n",
    "        if fn:\n",
    "            yield fn, part.get_payload()\n",
    "\n",
    "# ─── helper: call GLM ───────────────────────────────────────────────────\n",
    "def glm(prompt:str)->str:\n",
    "    res = requests.post(\n",
    "        GLM_URL,\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\":[\n",
    "                {\"role\":\"system\",\n",
    "                 \"content\":\"请仅用 JSON 数组返回基金净值指标（日期、基金名称、基金代码、单位净值、累计净值、发件机构）。\"},\n",
    "                {\"role\":\"user\",\"content\":prompt}],\n",
    "            \"temperature\":0.2,\n",
    "            \"max_tokens\":32000,\n",
    "            \"stream\":False},\n",
    "        headers={\"Authorization\":f\"Bearer {GLM_KEY}\"},\n",
    "        timeout=300)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def parse_glm(txt:str):\n",
    "    try:\n",
    "        data=json.loads(txt)\n",
    "        return data if isinstance(data,list) else [data]\n",
    "    except Exception:\n",
    "        return [dict(zip(COLS,m)) for m in RX.findall(txt)]\n",
    "\n",
    "# ─── main workflow ──────────────────────────────────────────────────────\n",
    "def main():\n",
    "    rows=[]; idx=1\n",
    "    for msg in fetch_mail():\n",
    "        sender_name,sender_email = msg.get_addresses(\"from\")[0]\n",
    "        subj   = msg.get_subject()\n",
    "        body   = get_body(msg)\n",
    "        atts   = list(list_attachments(msg))\n",
    "\n",
    "        print(f\"\\n[{idx}] {subj}\\n    From: {sender_name} <{sender_email}>\\n\"\n",
    "              f\"    Attachments ({len(atts)}): {[fn for fn,_ in atts]}\")\n",
    "        idx+=1\n",
    "\n",
    "        # Always process body alone (even if no attachment)\n",
    "        payloads = atts if atts else [(None, b\"\")]\n",
    "\n",
    "        for fn,blob in payloads:\n",
    "            attach_text=\"(无附件)\"\n",
    "            if fn:                                   # we have bytes\n",
    "                with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "                    tmp.write(blob); tmp.flush()\n",
    "                    # try excel → csv\n",
    "                    try:\n",
    "                        df = pd.read_excel(tmp.name)  # full sheet(s)\n",
    "                        attach_text = df.to_csv(index=False)\n",
    "                    except Exception:\n",
    "                        try:                          # fallback plain text\n",
    "                            attach_text = blob.decode(\"utf-8\", \"ignore\")\n",
    "                        except Exception:\n",
    "                            attach_text = \"(binary 文件预览省略)\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"邮件主题: {subj}\\n\"\n",
    "                f\"发件人: {sender_name}\\n\\n\"\n",
    "                f\"【邮件正文】\\n{body}\\n\\n\"\n",
    "                f\"【附件: {fn or '无'}】\\n{attach_text}\"\n",
    "            )\n",
    "\n",
    "            ans = glm(prompt)\n",
    "            parsed = parse_glm(ans)\n",
    "\n",
    "            if parsed:\n",
    "                print(f\"    ↪ GLM parsed {len(parsed)} row(s) \"\n",
    "                      f\"from {'正文' if fn is None else fn}\")\n",
    "                for item in parsed:\n",
    "                    row={c:item.get(c,\"\") for c in COLS}\n",
    "                    row.update({\"原邮件名\":subj,\n",
    "                                \"发件人\":sender_email,\n",
    "                                \"发件机构\":sender_name})\n",
    "                    rows.append(row)\n",
    "            else:\n",
    "                print(f\"    ↪ 0 rows parsed from \"\n",
    "                      f\"{'正文' if fn is None else fn}\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"\\n👀 Finished – no NAV data captured.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    append_mode = pathlib.Path(XLSX).exists()\n",
    "    writer_kwargs = dict(engine=\"openpyxl\", mode=\"a\" if append_mode else \"w\")\n",
    "    if append_mode:\n",
    "        writer_kwargs[\"if_sheet_exists\"] = \"replace\"\n",
    "\n",
    "    with pd.ExcelWriter(XLSX, **writer_kwargs) as xw:\n",
    "        df.to_excel(xw, index=False, sheet_name=SHEET)\n",
    "\n",
    "    print(f\"\\n✅ {len(df)} rows written → {XLSX}\")\n",
    "\n",
    "# ─── run ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    with contextlib.suppress(KeyboardInterrupt):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a2a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📬 5 messages since 2025-04-27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] 恭喜，手机号码邮箱已开通\n",
      "    From: 网易邮件中心 <mail@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取其中的公募基金或私募基金的净值信息。首先，我要仔细阅读邮件主题和正文，看看是否有任何关于基金净值的数据。\n",
      "\n",
      "邮件主题是“恭喜，手机号码邮箱已...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "\n",
      "[2] 邮件办公，如此轻松！欢迎来到网易邮箱！\n",
      "    From: 网易邮箱助手 <club@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，我要仔细阅读邮件主题、正文和附件的信息。\n",
      "\n",
      "邮件主题是“邮件办公，如此轻松！欢迎来到网易邮箱！”，看起来...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "\n",
      "[3] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['Book2.xlsx']\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，用户给了一个邮件主题和正文，还有附件，但看起来正文内容是“sasasaswtest”，主题是“Sssa...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，我要仔细阅读邮件主题、正文和附件内容，看看是否有相关的基金净值数据。\n",
      "\n",
      "邮件主题是“Sssa test2...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from Book2.xlsx\n",
      "\n",
      "[4] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (2): ['Book2.xlsx', 'SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls']\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，我要仔细阅读用户的要求，确保完全理解任务。用户希望从邮件主题、正文和附件中识别出基金净值数据，并以特定的...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我需要仔细分析用户提供的邮件内容，提取关于公募基金或私募基金的净值信息。首先，用户要求从邮件主题、正文和附件中识别相关数据，并以JSON数组形式返回，每个对象包含日期、基金名称、...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from Book2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募或私募基金的净值信息。首先，用户给了一个具体的例子，我需要仔细分析邮件的各个部分，确保正确提取数据。\n",
      "\n",
      "首先看邮件主题是“Sssa te...'\n",
      "    ↪ GLM parsed 1 row(s) from SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls\n",
      "\n",
      "[5] Sssa test2基金净值净值\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls']\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，用户给了一个邮件主题和正文，还有附件，但看起来正文内容是“sasasaswtest”，看起来可能没有实际...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|██████████| 5/5 [00:34<00:00,  6.86s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募或私募基金的净值信息。首先，我要仔细阅读邮件主题、正文和附件的信息。\n",
      "\n",
      "邮件主题是“Sssa test2基金净值净值”，看起来可能包含基...'\n",
      "    ↪ GLM parsed 1 row(s) from SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls\n",
      "\n",
      "✅ 2 unique rows written → 2025-05-27 基金净值.xlsx (Sheet: 2025-05-27)\n",
      "\n",
      "👋 Script finished or interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fund-NAV harvester v0.9.2 (LLM-focused, improved parsing & prompt)\n",
    "───────────────────────────────────────────────────────────────────────────\n",
    "1. IMAP login (163.com, ID handshake)\n",
    "2. For each message:\n",
    "      • capture subject + sender + full body text\n",
    "      • capture every attachment (any filename)\n",
    "      • send ⟨subject + body + attachment text⟩ to GLM-Z1-Flash\n",
    "3. Parse LLM's JSON response & write rows →  YYYY-MM-DD 基金净值.xlsx\n",
    "\"\"\"\n",
    "\n",
    "import re, json, tempfile, pathlib, datetime, contextlib, io, warnings\n",
    "from imapclient import IMAPClient\n",
    "import pyzmail, pandas as pd, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional, nicer HTML-to-text if bs4 is around\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    def html2text(html:str)->str:\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\")\n",
    "except ImportError:\n",
    "    def html2text(html:str)->str:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", html)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# ─── creds & endpoints ──────────────────────────────────────────────────\n",
    "IMAP_HOST  = \"imap.163.com\"\n",
    "EMAIL_USER = \"zhanluekehu@163.com\" # Replace with your actual email\n",
    "EMAIL_PWD  = \"DRqdN38whrnCFPGx\"    # Replace with your actual 163 App Authorization Code\n",
    "GLM_KEY    = \"afe7583d73c9d3948f60230e79e08151.Z9HPB84mxuC31DeK\" # Replace with your actual GLM API Key\n",
    "GLM_URL    = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "MODEL      = \"glm-z1-flash\" # Or your preferred model like \"glm-4\", \"glm-3-turbo\"\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "TODAY   = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "XLSX    = f\"{TODAY} 基金净值.xlsx\"\n",
    "SHEET   = TODAY\n",
    "COLS    = [\"日期\",\"基金名称\",\"基金代码\",\"单位净值\",\"累计净值\",\n",
    "           \"原邮件名\",\"发件人\",\"发件机构\"]\n",
    "\n",
    "# ─── helper: fetch recent mail ──────────────────────────────────────────\n",
    "def fetch_mail(days:int=30):\n",
    "    with IMAPClient(IMAP_HOST, ssl=True) as srv:\n",
    "        srv.login(EMAIL_USER, EMAIL_PWD)\n",
    "        try:\n",
    "            srv.id_({\"name\":\"python\",\"version\":\"0.9.2\",\"vendor\":\"myclient\",\n",
    "                     \"contact\":EMAIL_USER})\n",
    "        except Exception:\n",
    "            pass \n",
    "        srv.select_folder(\"INBOX\")\n",
    "        since_date = (datetime.date.today()-datetime.timedelta(days=days))\n",
    "        search_criteria = [\"SINCE\", since_date.strftime(\"%d-%b-%Y\")]\n",
    "        \n",
    "        ids = srv.search(search_criteria)\n",
    "        print(f\"📬 {len(ids)} messages since {since_date.strftime('%Y-%m-%d')}\\n\")\n",
    "        if not ids:\n",
    "            return\n",
    "\n",
    "        for mid in tqdm(ids, desc=\"Fetching\", unit=\"mail\"):\n",
    "            raw_email_data = srv.fetch([mid], [\"RFC822\"])[mid]\n",
    "            if b\"RFC822\" in raw_email_data:\n",
    "                yield pyzmail.PyzMessage.factory(raw_email_data[b\"RFC822\"])\n",
    "            else:\n",
    "                print(f\"Warning: Could not fetch RFC822 data for message ID {mid}\")\n",
    "\n",
    "# ─── helper: full body text ─────────────────────────────────────────────\n",
    "def get_body(msg):\n",
    "    if msg.text_part:\n",
    "        charset = msg.text_part.charset or \"utf-8\"\n",
    "        return msg.text_part.get_payload().decode(charset, \"ignore\")\n",
    "    if msg.html_part:\n",
    "        charset = msg.html_part.charset or \"utf-8\"\n",
    "        html = msg.html_part.get_payload().decode(charset, \"ignore\")\n",
    "        return html2text(html)\n",
    "    return \"\"\n",
    "\n",
    "# ─── helper: list attachments (filename, bytes) ─────────────────────────\n",
    "def list_attachments(msg):\n",
    "    for part in msg.mailparts:\n",
    "        fn = getattr(part, \"filename\", None)\n",
    "        if fn:\n",
    "            payload_bytes = part.get_payload()\n",
    "            if not isinstance(payload_bytes, bytes):\n",
    "                charset = part.charset or \"utf-8\"\n",
    "                try:\n",
    "                    payload_bytes = str(payload_bytes).encode(charset, \"ignore\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠️ Could not encode attachment '{fn}' payload to bytes: {e}. Skipping.\")\n",
    "                    continue\n",
    "            yield fn, payload_bytes\n",
    "\n",
    "# ─── helper: call GLM ───────────────────────────────────────────────────\n",
    "def glm(prompt:str)->str:\n",
    "    system_prompt = \"\"\"您是一位提取金融数据的专家。请从提供的文本（邮件主题、正文和附件）中识别并提取关于公募基金或私募基金的净值信息。\n",
    "请将信息以 JSON 对象数组的形式返回。每个对象应代表一只独立的基金，并精确包含以下字段：\n",
    "- \"日期\": 基金净值的日期，格式为YYYY-MM-DD，来源于文本内容。\n",
    "- \"基金名称\": 基金的名称。\n",
    "- \"基金代码\": 基金的字母或数字代码。\n",
    "- \"单位净值\": 单位净值，应为一个数字。\n",
    "- \"累计净值\": 累计净值，应为一个数字。\n",
    "\n",
    "重要提示：\n",
    "- 仅包含明确的基金净值数据条目。\n",
    "- 如果列出了多只基金，请为每只基金创建一个单独的 JSON 对象。\n",
    "- 如果在文本中未找到有效的基金净值数据，请返回一个空的 JSON 数组：[]。\n",
    "- **您的回复必须严格遵守输出格式。您的回复只能包含一个 JSON 对象数组，不能有任何其他文字、解释、注释或思考过程。绝对不要使用 `<think>` 或任何类似的标签。如果找不到数据，请返回空的 JSON 数组 `[]`。任何偏离此 JSON-only 格式的输出都将被视为失败。**\n",
    "- 确保“单位净值”和“累计净值”的值是数字。\n",
    "- 请仔细准确识别基金名称和代码，避免提取通用文本或文件名。\n",
    "- “日期”应该是与净值相关的特定日期，除非明确说明是净值日期，否则不一定是邮件日期或报告生成日期。\n",
    "\n",
    "期望的单个基金输出示例：\n",
    "[\n",
    "  {\n",
    "    \"日期\": \"2025-05-26\",\n",
    "    \"基金名称\": \"九招真格量化套利一号私募证券投资基金\",\n",
    "    \"基金代码\": \"SQD546\",\n",
    "    \"单位净值\": 1.0580,\n",
    "    \"累计净值\": 1.5053\n",
    "  }\n",
    "]\n",
    "无数据时输出示例：\n",
    "[]\n",
    "\"\"\"\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            GLM_URL,\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\":[\n",
    "                    {\"role\":\"system\", \"content\": system_prompt},\n",
    "                    {\"role\":\"user\",\"content\":prompt}],\n",
    "                \"temperature\":0.2,\n",
    "                \"max_tokens\":32000,\n",
    "                \"stream\":False},\n",
    "            headers={\"Authorization\":f\"Bearer {GLM_KEY}\"},\n",
    "            timeout=300)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"    ‼️ GLM API request failed: {e}\")\n",
    "        return \"[]\" \n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        response_text = res.text if 'res' in locals() else \"N/A (response object not available)\"\n",
    "        print(f\"    ‼️ GLM API response format unexpected or not valid JSON: {e} - Response: {response_text[:200]}\")\n",
    "        return \"[]\"\n",
    "\n",
    "def parse_glm(txt:str):\n",
    "    try:\n",
    "        cleaned_txt = txt.strip()\n",
    "\n",
    "        json_start_index = -1\n",
    "        first_brace = cleaned_txt.find('{')\n",
    "        first_bracket = cleaned_txt.find('[')\n",
    "\n",
    "        if first_brace != -1 and first_bracket != -1:\n",
    "            json_start_index = min(first_brace, first_bracket)\n",
    "        elif first_brace != -1:\n",
    "            json_start_index = first_brace\n",
    "        elif first_bracket != -1:\n",
    "            json_start_index = first_bracket\n",
    "        \n",
    "        if json_start_index > 0:\n",
    "            preceding_text = cleaned_txt[:json_start_index]\n",
    "            if \"<think>\" in preceding_text.lower(): \n",
    "                 print(f\"    ℹ️ Stripped preceding LLM thought process/text: '{preceding_text[:100].strip()}...'\")\n",
    "            else:\n",
    "                 print(f\"    ℹ️ Stripped preceding non-JSON text: '{preceding_text[:100].strip()}...'\")\n",
    "            cleaned_txt = cleaned_txt[json_start_index:]\n",
    "        elif json_start_index == -1 :\n",
    "            if \"<think>\" in cleaned_txt.lower() :\n",
    "                 print(f\"    ⚠️ GLM output appears to be only thought process/text without JSON: '{cleaned_txt[:200].strip()}...'\")\n",
    "            else:\n",
    "                 print(f\"    ⚠️ GLM output does not contain valid JSON start character ([ or {{): '{cleaned_txt[:200].strip()}...'\")\n",
    "            return []\n",
    "\n",
    "        if cleaned_txt.startswith(\"```json\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```json\"):].strip()\n",
    "        elif cleaned_txt.startswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```\"):].strip()\n",
    "        if cleaned_txt.endswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[:-len(\"```\")].strip()\n",
    "\n",
    "        if not cleaned_txt:\n",
    "            return []\n",
    "        \n",
    "        data = json.loads(cleaned_txt)\n",
    "        \n",
    "        parsed_items = []\n",
    "        expected_keys = {\"日期\", \"基金名称\", \"基金代码\", \"单位净值\", \"累计净值\"}\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and expected_keys.issubset(item.keys()):\n",
    "                    try:\n",
    "                        item[\"单位净值\"] = float(str(item[\"单位净值\"]).replace(',',''))\n",
    "                        item[\"累计净值\"] = float(str(item[\"累计净值\"]).replace(',',''))\n",
    "                        parsed_items.append(item)\n",
    "                    except (ValueError, TypeError):\n",
    "                        print(f\"    ⚠️ GLM list item skipped (net values not convertible to float): {str(item)[:100]}\")\n",
    "                elif isinstance(item, dict):\n",
    "                     print(f\"    ⚠️ GLM list item skipped (missing expected keys): {str(item)[:100]}\")\n",
    "                else:\n",
    "                    print(f\"    ⚠️ GLM list item skipped (not a dictionary): {str(item)[:100]}\")\n",
    "            return parsed_items\n",
    "        elif isinstance(data, dict): \n",
    "            if expected_keys.issubset(data.keys()):\n",
    "                try:\n",
    "                    data[\"单位净值\"] = float(str(data[\"单位净值\"]).replace(',',''))\n",
    "                    data[\"累计净值\"] = float(str(data[\"累计净值\"]).replace(',',''))\n",
    "                    return [data] \n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"    ⚠️ GLM dict item skipped (net values not convertible to float): {str(data)[:100]}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"    ⚠️ GLM dict skipped (missing expected keys): {str(data)[:100]}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"    ⚠️ GLM output (after stripping) is valid JSON but not a list or dict: {cleaned_txt[:200]}\")\n",
    "            return []\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"    ⚠️ GLM output (after stripping) was not valid JSON. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️ Unexpected error parsing GLM output: {e}. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "\n",
    "# ─── main workflow ──────────────────────────────────────────────────────\n",
    "def main():\n",
    "    rows=[]\n",
    "    mail_fetch_iterator = fetch_mail()\n",
    "    if mail_fetch_iterator is None:\n",
    "        print(\"\\n👀 No messages found based on search criteria.\")\n",
    "        return\n",
    "\n",
    "    for idx, msg in enumerate(mail_fetch_iterator, 1):\n",
    "        if msg is None: continue\n",
    "\n",
    "        sender_addresses = msg.get_addresses(\"from\")\n",
    "        if sender_addresses:\n",
    "            sender_name, sender_email = sender_addresses[0]\n",
    "        else:\n",
    "            sender_name, sender_email = \"Unknown Sender\", \"unknown@example.com\"\n",
    "\n",
    "        subj = msg.get_subject() or \"(No Subject)\"\n",
    "        body = get_body(msg)\n",
    "        atts = list(list_attachments(msg))\n",
    "\n",
    "        print(f\"\\n[{idx}] {subj}\\n    From: {sender_name} <{sender_email}>\\n\"\n",
    "              f\"    Attachments ({len(atts)}): {[fn for fn,_ in atts]}\")\n",
    "\n",
    "        payloads_to_process = [(None, b\"\")] \n",
    "        payloads_to_process.extend(atts)\n",
    "\n",
    "        for fn, blob in payloads_to_process:\n",
    "            attach_text = \"(无相关文本内容)\"\n",
    "            source_name = \"正文\"\n",
    "\n",
    "            if fn: \n",
    "                source_name = fn\n",
    "                temp_file_path = None\n",
    "                try:\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix=pathlib.Path(fn).suffix) as tmp:\n",
    "                        tmp.write(blob)\n",
    "                        temp_file_path = tmp.name\n",
    "                    \n",
    "                    try:\n",
    "                        xls_content = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "                        if isinstance(xls_content, dict):\n",
    "                            combined_df = pd.concat(xls_content.values(), ignore_index=True)\n",
    "                        else:\n",
    "                            combined_df = xls_content\n",
    "                        attach_text = combined_df.to_csv(index=False, header=True)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            attach_text = blob.decode(\"utf-8\", \"ignore\")\n",
    "                        except UnicodeDecodeError:\n",
    "                             attach_text = blob.decode(\"gbk\", \"ignore\") \n",
    "                        except Exception:\n",
    "                            attach_text = \"(二进制文件或无法识别编码)\"\n",
    "                except Exception as e_file:\n",
    "                    attach_text = f\"(附件处理错误: {e_file})\"\n",
    "                finally:\n",
    "                    if temp_file_path and pathlib.Path(temp_file_path).exists():\n",
    "                        pathlib.Path(temp_file_path).unlink()\n",
    "            \n",
    "            if fn is None: \n",
    "                prompt_context = f\"【邮件正文】\\n{body}\\n\\n\"\n",
    "            else: \n",
    "                prompt_context = f\"【邮件正文】\\n{body}\\n\\n【附件: {fn}】\\n{attach_text}\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"邮件主题: {subj}\\n\"\n",
    "                f\"发件人: {sender_name} <{sender_email}>\\n\\n\"\n",
    "                f\"{prompt_context}\"\n",
    "            )\n",
    "            \n",
    "            ans = glm(prompt)\n",
    "            parsed = parse_glm(ans)\n",
    "\n",
    "            if parsed:\n",
    "                print(f\"    ↪ GLM parsed {len(parsed)} row(s) from {source_name}\")\n",
    "                for item in parsed:\n",
    "                    row = {c: \"\" for c in COLS}\n",
    "                    row.update(item) \n",
    "                    row.update({\n",
    "                        \"原邮件名\": subj,\n",
    "                        \"发件人\": sender_email,\n",
    "                        \"发件机构\": sender_name\n",
    "                    })\n",
    "                    rows.append(row)\n",
    "            else:\n",
    "                print(f\"    ↪ 0 rows parsed (or parsing failed) from {source_name}\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"\\n👀 Finished – no NAV data captured.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    if df.empty:\n",
    "        print(\"\\n👀 No unique NAV data captured after processing and removing duplicates.\")\n",
    "        return\n",
    "    \n",
    "    file_exists = pathlib.Path(XLSX).exists()\n",
    "    excel_writer_mode = \"a\" if file_exists else \"w\"\n",
    "    excel_if_sheet_exists = \"replace\" if file_exists else None\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(XLSX, engine=\"openpyxl\", mode=excel_writer_mode, \n",
    "                            if_sheet_exists=excel_if_sheet_exists) as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name=SHEET, header=True)\n",
    "        print(f\"\\n✅ {len(df)} unique rows written → {XLSX} (Sheet: {SHEET})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ‼️ Error writing to Excel '{XLSX}': {e}.\")\n",
    "        fallback_xlsx = f\"{pathlib.Path(XLSX).stem}_fallback{pathlib.Path(XLSX).suffix}\"\n",
    "        try:\n",
    "            df.to_excel(fallback_xlsx, index=False, sheet_name=SHEET)\n",
    "            print(f\"\\n⚠️ Data saved to fallback file: {fallback_xlsx}\")\n",
    "        except Exception as fe:\n",
    "            print(f\"    ‼️ Error writing to fallback Excel file '{fallback_xlsx}': {fe}.\")\n",
    "\n",
    "# ─── run ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    with contextlib.suppress(KeyboardInterrupt):\n",
    "        main()\n",
    "    print(\"\\n👋 Script finished or interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01f8e64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Last run timestamp file not found. Processing with default window.\n",
      "📬 Found 7 email candidates from last 30 days (server search from date: 2025-04-27).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:   0%|          | 0/7 [00:00<?, ?mail/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Processing: 恭喜，手机号码邮箱已开通\n",
      "    From: 网易邮件中心 <mail@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  14%|█▍        | 1/7 [00:02<00:14,  2.36s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，并从中提取公募基金或私募基金的净值信息。首先，我要仔细阅读邮件主题和正文，看看是否有任何关于基金净值的数据。\n",
      "\n",
      "邮件主题是“恭喜，手机号码邮箱已...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "\n",
      "[2] Processing: 邮件办公，如此轻松！欢迎来到网易邮箱！\n",
      "    From: 网易邮箱助手 <club@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  29%|██▊       | 2/7 [00:05<00:15,  3.08s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取其中的公募基金或私募基金的净值信息。首先，我要仔细阅读用户的要求，确保完全理解任务。用户希望从邮件主题、正文和附件中识别出基金净值数据，并以...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "\n",
      "[3] Processing: Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['Book2.xlsx']\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，用户给了一个邮件主题和正文，还有附件，但看起来正文内容是“sasasaswtest”，主题是“Sssa...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  43%|████▎     | 3/7 [00:12<00:18,  4.71s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，从中提取公募基金或私募基金的净值信息。首先，我要仔细阅读邮件主题、正文和附件内容，看看是否有相关的基金净值数据。\n",
      "\n",
      "邮件主题是“Sssa tes...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from Book2.xlsx\n",
      "\n",
      "[4] Processing: Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (2): ['Book2.xlsx', 'SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls']\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，我要仔细阅读用户的要求，确保完全理解任务。用户希望从邮件主题、正文和附件中识别出基金净值数据，并以严格的...'\n",
      "    ⚠️ GLM output (after stripping) was not valid JSON. Original start: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，我要仔细阅读用户的要求，确保完全理解任务。用户希望从邮件主题、正文和附件中识别出基金净值数据，并以严格的...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，从中提取公募基金或私募基金的净值信息。首先，我要仔细阅读邮件主题、正文和附件的内容，看看是否有相关的基金净值数据。\n",
      "\n",
      "邮件主题是“Sssa te...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from Book2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  57%|█████▋    | 4/7 [00:25<00:23,  7.80s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募或私募基金的净值信息。首先，用户要求从邮件主题、正文和附件中识别相关数据，并按照严格的JSON格式返回。\n",
      "\n",
      "首先看邮件主题是“Sssa...'\n",
      "    ↪ GLM parsed 1 row(s) from SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls\n",
      "\n",
      "[5] Processing: Sssa test2基金净值净值\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls']\n",
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募基金或私募基金的净值信息。首先，用户给了一个邮件主题和正文，还有附件，但看起来正文内容是“sasasaswtest”，主题是“Sssa...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  71%|███████▏  | 5/7 [00:32<00:15,  7.54s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募或私募基金的净值信息。首先，用户给了一个具体的例子，我需要严格按照他们的要求来操作。\n",
      "\n",
      "首先看邮件主题是“Sssa test2基金净值净...'\n",
      "    ↪ GLM parsed 1 row(s) from SQD546_九招真格量化套利一号私募证券投资基金_2025-05-26_净值表.xls\n",
      "\n",
      "[6] Processing: 办公室共享了文件给您\n",
      "    From: svyxvl@sjwdif8f.cn <svyxvl@sjwdif8f.cn>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  86%|████████▌ | 6/7 [00:34<00:05,  5.78s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我现在需要处理用户提供的邮件内容，提取公募或私募基金的净值信息。首先，我要仔细阅读邮件主题、正文和附件的信息。邮件主题是“办公室共享了文件给您”，发件人地址看起来有点奇怪，可能是...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "\n",
      "[7] Processing: 【国信托管】您管理的以下产品于2025年05月27日 临时开放，烦请您向全体投资人披露该临时开放日事项\n",
      "    From: gxtggzhs@guosen.com.cn <gxtggzhs@guosen.com.cn>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering: 100%|██████████| 7/7 [00:37<00:00,  5.31s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ℹ️ Stripped preceding LLM thought process/text: '<think>\n",
      "好的，我需要仔细分析用户提供的邮件内容，提取关于公募基金或私募基金的净值信息。首先，用户要求只提取明确的净值数据，并且每个基金要单独作为一个JSON对象。如果没找到数据，就返回空数组。...'\n",
      "    ↪ 0 rows parsed (or parsing failed) from 正文\n",
      "\n",
      "✅ 2 unique rows written/updated → 2025-05-27 基金净值.xlsx (Sheet: 2025-05-27)\n",
      "☑️ Saved current run timestamp: 2025-05-27 15:25:20 UTC to log/last_run.txt\n",
      "\n",
      "👋 Script execution cycle finished or was terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fund-NAV harvester v0.9.3 (LLM-focused, incremental processing, improved parsing & prompt)\n",
    "───────────────────────────────────────────────────────────────────────────\n",
    "1. IMAP login (163.com, ID handshake)\n",
    "2. Read last run timestamp.\n",
    "3. For each new message (since last run):\n",
    "     • capture subject + sender + full body text\n",
    "     • capture every attachment (any filename)\n",
    "     • send ⟨subject + body + attachment text⟩ to GLM-Z1-Flash\n",
    "4. Parse LLM's JSON response & write rows → YYYY-MM-DD 基金净值.xlsx\n",
    "5. Save current run timestamp.\n",
    "\"\"\"\n",
    "\n",
    "import re, json, tempfile, pathlib, datetime, contextlib, io, warnings\n",
    "from imapclient import IMAPClient\n",
    "import pyzmail, pandas as pd, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional, nicer HTML-to-text if bs4 is around\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    def html2text(html:str)->str:\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\")\n",
    "except ImportError:\n",
    "    def html2text(html:str)->str:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", html)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# ─── creds & endpoints ──────────────────────────────────────────────────\n",
    "IMAP_HOST  = \"imap.163.com\"\n",
    "EMAIL_USER = \"zhanluekehu@163.com\" # Replace with your actual email\n",
    "EMAIL_PWD  = \"DRqdN38whrnCFPGx\"    # Replace with your actual 163 App Authorization Code\n",
    "GLM_KEY    = \"afe7583d73c9d3948f60230e79e08151.Z9HPB84mxuC31DeK\" # Replace with your actual GLM API Key\n",
    "GLM_URL    = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "MODEL      = \"glm-z1-flash\" # Or your preferred model like \"glm-4\", \"glm-3-turbo\"\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "TODAY   = datetime.date.today().strftime(\"%Y-%m-%d\") # Used for default Excel sheet name\n",
    "XLSX    = f\"{TODAY} 基金净值.xlsx\" # Output Excel filename uses current date\n",
    "SHEET   = TODAY # Sheet name is current date\n",
    "COLS    = [\"日期\",\"基金名称\",\"基金代码\",\"单位净值\",\"累计净值\",\n",
    "           \"原邮件名\",\"发件人\",\"发件机构\"]\n",
    "\n",
    "# ─── Timestamp logging for incremental processing ─────────────────────\n",
    "LOG_DIR = pathlib.Path(\"log\")\n",
    "LAST_RUN_FILE = LOG_DIR / \"last_run.txt\"\n",
    "DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\" # UTC datetime format for the log file\n",
    "\n",
    "def get_last_run_datetime() -> datetime.datetime | None:\n",
    "    \"\"\"Reads the last successful run datetime from the log file (expects UTC).\"\"\"\n",
    "    if not LAST_RUN_FILE.exists():\n",
    "        print(\"ℹ️ Last run timestamp file not found. Processing with default window.\")\n",
    "        return None\n",
    "    try:\n",
    "        content = LAST_RUN_FILE.read_text().strip()\n",
    "        if not content:\n",
    "            print(\"ℹ️ Last run timestamp file is empty. Processing with default window.\")\n",
    "            return None\n",
    "        dt_naive = datetime.datetime.strptime(content, DATETIME_FORMAT)\n",
    "        # Assume stored time is UTC, make it timezone-aware\n",
    "        dt_utc = dt_naive.replace(tzinfo=datetime.timezone.utc)\n",
    "        print(f\"ℹ️ Previous run timestamp: {dt_utc.strftime(DATETIME_FORMAT)} UTC\")\n",
    "        return dt_utc\n",
    "    except (ValueError, OSError) as e:\n",
    "        print(f\"⚠️ Error reading or parsing last run timestamp from {LAST_RUN_FILE}: {e}. Processing with default window.\")\n",
    "        return None\n",
    "\n",
    "def save_current_run_datetime():\n",
    "    \"\"\"Saves the current datetime (UTC) as the last successful run timestamp.\"\"\"\n",
    "    try:\n",
    "        LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        now_utc = datetime.datetime.now(datetime.timezone.utc)\n",
    "        LAST_RUN_FILE.write_text(now_utc.strftime(DATETIME_FORMAT))\n",
    "        print(f\"☑️ Saved current run timestamp: {now_utc.strftime(DATETIME_FORMAT)} UTC to {LAST_RUN_FILE}\")\n",
    "    except OSError as e:\n",
    "        print(f\"⚠️ Could not save current run timestamp to {LAST_RUN_FILE}: {e}\")\n",
    "\n",
    "# ─── helper: fetch mail (modified for incremental processing) ───────────\n",
    "def fetch_mail(last_run_utc_dt: datetime.datetime | None = None, default_days_lookback: int = 30):\n",
    "    \"\"\"\n",
    "    Fetches emails. If last_run_utc_dt is provided, fetches emails SINCE that date\n",
    "    and then filters by time. Otherwise, fetches emails from default_days_lookback.\n",
    "    Yields pyzmail.PyzMessage objects.\n",
    "    \"\"\"\n",
    "    with IMAPClient(IMAP_HOST, ssl=True) as srv:\n",
    "        srv.login(EMAIL_USER, EMAIL_PWD)\n",
    "        try:\n",
    "            srv.id_({\"name\":\"python\",\"version\":\"0.9.3\",\"vendor\":\"myclient\", # Updated version\n",
    "                     \"contact\":EMAIL_USER})\n",
    "        except Exception:\n",
    "            pass # Optional, continue if ID command fails\n",
    "        \n",
    "        srv.select_folder(\"INBOX\")\n",
    "        \n",
    "        search_description = \"\"\n",
    "        using_last_run_filter = False\n",
    "\n",
    "        if last_run_utc_dt:\n",
    "            # IMAP SINCE uses date part. Server returns all emails on or after this date.\n",
    "            # Time-based filtering will be done client-side using INTERNALDATE.\n",
    "            # Ensure last_run_utc_dt is UTC-aware for comparison.\n",
    "            if last_run_utc_dt.tzinfo is None or last_run_utc_dt.tzinfo.utcoffset(last_run_utc_dt) is None:\n",
    "                last_run_utc_dt = last_run_utc_dt.replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "            since_date_for_imap = last_run_utc_dt.date()\n",
    "            search_criteria = [\"SINCE\", since_date_for_imap]\n",
    "            search_description = (f\"candidates since {last_run_utc_dt.strftime(DATETIME_FORMAT)} UTC \"\n",
    "                                  f\"(server search from date: {since_date_for_imap.strftime('%Y-%m-%d')})\")\n",
    "            using_last_run_filter = True\n",
    "        else:\n",
    "            # Fallback to default lookback period if no last run timestamp\n",
    "            since_date_for_imap = (datetime.datetime.now(datetime.timezone.utc).date() - \n",
    "                                   datetime.timedelta(days=default_days_lookback))\n",
    "            search_criteria = [\"SINCE\", since_date_for_imap] # imapclient handles date obj\n",
    "            search_description = (f\"candidates from last {default_days_lookback} days \"\n",
    "                                  f\"(server search from date: {since_date_for_imap.strftime('%Y-%m-%d')})\")\n",
    "\n",
    "        ids = srv.search(search_criteria)\n",
    "        print(f\"📬 Found {len(ids)} email {search_description}.\")\n",
    "        \n",
    "        if not ids:\n",
    "            print(\"No emails matched server-side criteria.\\n\")\n",
    "            return\n",
    "\n",
    "        for mid in tqdm(ids, desc=\"Fetching & Filtering\", unit=\"mail\", mininterval=0.5, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'):\n",
    "            raw_email_data_map = srv.fetch([mid], [\"RFC822\", \"INTERNALDATE\"])\n",
    "            \n",
    "            if not raw_email_data_map or mid not in raw_email_data_map:\n",
    "                tqdm.write(f\"Warning: Could not fetch full data for message ID {mid}\")\n",
    "                continue \n",
    "            \n",
    "            message_data = raw_email_data_map[mid]\n",
    "\n",
    "            if b\"RFC822\" not in message_data:\n",
    "                tqdm.write(f\"Warning: Could not fetch RFC822 (body) for message ID {mid}\")\n",
    "                continue\n",
    "\n",
    "            if using_last_run_filter:\n",
    "                internal_date_from_server = message_data.get(b'INTERNALDATE') # datetime obj from imapclient\n",
    "                \n",
    "                if internal_date_from_server:\n",
    "                    if internal_date_from_server.tzinfo is None or \\\n",
    "                       internal_date_from_server.tzinfo.utcoffset(internal_date_from_server) is None:\n",
    "                        internal_date_from_server = internal_date_from_server.replace(tzinfo=datetime.timezone.utc)\n",
    "                    \n",
    "                    if internal_date_from_server <= last_run_utc_dt:\n",
    "                        continue \n",
    "                else:\n",
    "                    tqdm.write(f\"Warning: Message ID {mid} missing INTERNALDATE. Cannot filter by exact time. Processing due to date match.\")\n",
    "\n",
    "            yield pyzmail.PyzMessage.factory(message_data[b\"RFC822\"])\n",
    "\n",
    "# ─── helper: full body text ─────────────────────────────────────────────\n",
    "def get_body(msg):\n",
    "    if msg.text_part:\n",
    "        charset = msg.text_part.charset or \"utf-8\"\n",
    "        return msg.text_part.get_payload().decode(charset, \"ignore\")\n",
    "    if msg.html_part:\n",
    "        charset = msg.html_part.charset or \"utf-8\"\n",
    "        html = msg.html_part.get_payload().decode(charset, \"ignore\")\n",
    "        return html2text(html)\n",
    "    return \"\"\n",
    "\n",
    "# ─── helper: list attachments (filename, bytes) ─────────────────────────\n",
    "def list_attachments(msg):\n",
    "    for part in msg.mailparts:\n",
    "        fn = getattr(part, \"filename\", None)\n",
    "        if fn:\n",
    "            payload_bytes = part.get_payload()\n",
    "            if not isinstance(payload_bytes, bytes):\n",
    "                charset = part.charset or \"utf-8\"\n",
    "                try:\n",
    "                    payload_bytes = str(payload_bytes).encode(charset, \"ignore\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠️ Could not encode attachment '{fn}' payload to bytes: {e}. Skipping.\")\n",
    "                    continue\n",
    "            yield fn, payload_bytes\n",
    "\n",
    "# ─── helper: call GLM ───────────────────────────────────────────────────\n",
    "def glm(prompt:str)->str:\n",
    "    system_prompt = \"\"\"您是一位提取金融数据的专家。请从提供的文本（邮件主题、正文和附件）中识别并提取关于公募基金或私募基金的净值信息。\n",
    "请将信息以 JSON 对象数组的形式返回。每个对象应代表一只独立的基金，并精确包含以下字段：\n",
    "- \"日期\": 基金净值的日期，格式为YYYY-MM-DD，来源于文本内容。\n",
    "- \"基金名称\": 基金的名称。\n",
    "- \"基金代码\": 基金的字母数字代码。\n",
    "- \"单位净值\": 单位净值，应为一个数字。\n",
    "- \"累计净值\": 累计净值，应为一个数字。\n",
    "\n",
    "重要提示：\n",
    "- 仅包含明确的基金净值数据条目。\n",
    "- 如果列出了多只基金，请为每只基金创建一个单独的 JSON 对象。\n",
    "- 如果在文本中未找到有效的基金净值数据，请返回一个空的 JSON 数组：[]。\n",
    "- **您的回复必须严格遵守输出格式。您的回复只能包含一个 JSON 对象数组，不能有任何其他文字、解释、注释或思考过程。绝对不要使用 `<think>` 或任何类似的标签。如果找不到数据，请返回空的 JSON 数组 `[]`。任何偏离此 JSON-only 格式的输出都将被视为失败。**\n",
    "- 确保“单位净值”和“累计净值”的值是数字。\n",
    "- 请仔细准确识别基金名称和代码，避免提取通用文本或文件名。\n",
    "- “日期”应该是与净值相关的特定日期，除非明确说明是净值日期，否则不一定是邮件日期或报告生成日期。\n",
    "\n",
    "期望的单个基金输出示例：\n",
    "[\n",
    "  {\n",
    "    \"日期\": \"2025-05-26\",\n",
    "    \"基金名称\": \"九招真格量化套利一号私募证券投资基金\",\n",
    "    \"基金代码\": \"SQD546\",\n",
    "    \"单位净值\": 1.0580,\n",
    "    \"累计净值\": 1.5053\n",
    "  }\n",
    "]\n",
    "无数据时输出示例：\n",
    "[]\n",
    "\"\"\"\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            GLM_URL,\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\":[\n",
    "                    {\"role\":\"system\", \"content\": system_prompt},\n",
    "                    {\"role\":\"user\",\"content\":prompt}],\n",
    "                \"temperature\":0.2,\n",
    "                \"max_tokens\":32000, # Increased as per original example\n",
    "                \"stream\":False},\n",
    "            headers={\"Authorization\":f\"Bearer {GLM_KEY}\"},\n",
    "            timeout=300)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"    ‼️ GLM API request failed: {e}\")\n",
    "        return \"[]\" \n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        response_text = res.text if 'res' in locals() else \"N/A (response object not available)\"\n",
    "        print(f\"    ‼️ GLM API response format unexpected or not valid JSON: {e} - Response: {response_text[:200]}\")\n",
    "        return \"[]\"\n",
    "\n",
    "# ─── helper: parse GLM response ─────────────────────────────────────────\n",
    "def parse_glm(txt:str):\n",
    "    try:\n",
    "        cleaned_txt = txt.strip()\n",
    "\n",
    "        json_start_index = -1\n",
    "        first_brace = cleaned_txt.find('{')\n",
    "        first_bracket = cleaned_txt.find('[')\n",
    "\n",
    "        if first_brace != -1 and first_bracket != -1:\n",
    "            json_start_index = min(first_brace, first_bracket)\n",
    "        elif first_brace != -1:\n",
    "            json_start_index = first_brace\n",
    "        elif first_bracket != -1:\n",
    "            json_start_index = first_bracket\n",
    "        \n",
    "        if json_start_index > 0:\n",
    "            preceding_text = cleaned_txt[:json_start_index]\n",
    "            if \"<think>\" in preceding_text.lower(): \n",
    "                print(f\"    ℹ️ Stripped preceding LLM thought process/text: '{preceding_text[:100].strip()}...'\")\n",
    "            else:\n",
    "                print(f\"    ℹ️ Stripped preceding non-JSON text: '{preceding_text[:100].strip()}...'\")\n",
    "            cleaned_txt = cleaned_txt[json_start_index:]\n",
    "        elif json_start_index == -1 :\n",
    "            if \"<think>\" in cleaned_txt.lower() :\n",
    "                print(f\"    ⚠️ GLM output appears to be only thought process/text without JSON: '{cleaned_txt[:200].strip()}...'\")\n",
    "            else:\n",
    "                print(f\"    ⚠️ GLM output does not contain valid JSON start character ([ or {{): '{cleaned_txt[:200].strip()}...'\")\n",
    "            return []\n",
    "\n",
    "        if cleaned_txt.startswith(\"```json\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```json\"):].strip()\n",
    "        elif cleaned_txt.startswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```\"):].strip()\n",
    "        if cleaned_txt.endswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[:-len(\"```\")].strip()\n",
    "\n",
    "        if not cleaned_txt:\n",
    "            return []\n",
    "        \n",
    "        data = json.loads(cleaned_txt)\n",
    "        \n",
    "        parsed_items = []\n",
    "        expected_keys = {\"日期\", \"基金名称\", \"基金代码\", \"单位净值\", \"累计净值\"}\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and expected_keys.issubset(item.keys()):\n",
    "                    try:\n",
    "                        item[\"单位净值\"] = float(str(item[\"单位净值\"]).replace(',',''))\n",
    "                        item[\"累计净值\"] = float(str(item[\"累计净值\"]).replace(',',''))\n",
    "                        parsed_items.append(item)\n",
    "                    except (ValueError, TypeError):\n",
    "                        print(f\"    ⚠️ GLM list item skipped (net values not convertible to float): {str(item)[:100]}\")\n",
    "                elif isinstance(item, dict):\n",
    "                    print(f\"    ⚠️ GLM list item skipped (missing expected keys): {str(item)[:100]}\")\n",
    "                else:\n",
    "                    print(f\"    ⚠️ GLM list item skipped (not a dictionary): {str(item)[:100]}\")\n",
    "            return parsed_items\n",
    "        elif isinstance(data, dict): \n",
    "            if expected_keys.issubset(data.keys()):\n",
    "                try:\n",
    "                    data[\"单位净值\"] = float(str(data[\"单位净值\"]).replace(',',''))\n",
    "                    data[\"累计净值\"] = float(str(data[\"累计净值\"]).replace(',',''))\n",
    "                    return [data] \n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"    ⚠️ GLM dict item skipped (net values not convertible to float): {str(data)[:100]}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"    ⚠️ GLM dict skipped (missing expected keys): {str(data)[:100]}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"    ⚠️ GLM output (after stripping) is valid JSON but not a list or dict: {cleaned_txt[:200]}\")\n",
    "            return []\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"    ⚠️ GLM output (after stripping) was not valid JSON. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️ Unexpected error parsing GLM output: {e}. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "\n",
    "# ─── main workflow ──────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # Ensure log directory exists (also created by save_current_run_datetime if needed)\n",
    "    LOG_DIR.mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    last_run_dt_utc = get_last_run_datetime()\n",
    "    \n",
    "    rows = []\n",
    "    # Pass the last run datetime to fetch_mail; default_days_lookback is used if last_run_dt_utc is None\n",
    "    mail_fetch_iterator = fetch_mail(last_run_utc_dt=last_run_dt_utc, default_days_lookback=30)\n",
    "    \n",
    "    actual_emails_processed_count = 0\n",
    "    if mail_fetch_iterator:\n",
    "        for loop_idx, msg in enumerate(mail_fetch_iterator, 1):\n",
    "            actual_emails_processed_count = loop_idx \n",
    "            if msg is None: continue\n",
    "\n",
    "            sender_addresses = msg.get_addresses(\"from\")\n",
    "            if sender_addresses:\n",
    "                sender_name, sender_email = sender_addresses[0]\n",
    "            else:\n",
    "                sender_name, sender_email = \"Unknown Sender\", \"unknown@example.com\"\n",
    "\n",
    "            subj = msg.get_subject() or \"(No Subject)\"\n",
    "            body = get_body(msg)\n",
    "            atts = list(list_attachments(msg))\n",
    "\n",
    "            print(f\"\\n[{actual_emails_processed_count}] Processing: {subj}\\n    From: {sender_name} <{sender_email}>\\n\"\n",
    "                  f\"    Attachments ({len(atts)}): {[fn for fn,_ in atts]}\")\n",
    "\n",
    "            payloads_to_process = [(None, b\"\")] \n",
    "            payloads_to_process.extend(atts)\n",
    "\n",
    "            for fn, blob in payloads_to_process:\n",
    "                attach_text = \"(无相关文本内容)\"\n",
    "                source_name = \"正文\"\n",
    "\n",
    "                if fn: \n",
    "                    source_name = fn\n",
    "                    temp_file_path = None\n",
    "                    try:\n",
    "                        # Create a temporary file with the correct suffix for pandas to infer type\n",
    "                        with tempfile.NamedTemporaryFile(delete=False, suffix=pathlib.Path(fn).suffix) as tmp:\n",
    "                            tmp.write(blob)\n",
    "                            temp_file_path = tmp.name\n",
    "                        \n",
    "                        try:\n",
    "                            xls_content = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "                            if isinstance(xls_content, dict):\n",
    "                                combined_df = pd.concat(xls_content.values(), ignore_index=True)\n",
    "                            else:\n",
    "                                combined_df = xls_content\n",
    "                            attach_text = combined_df.to_csv(index=False, header=True)\n",
    "                        except Exception:\n",
    "                            try:\n",
    "                                attach_text = blob.decode(\"utf-8\", \"ignore\")\n",
    "                            except UnicodeDecodeError:\n",
    "                                attach_text = blob.decode(\"gbk\", \"ignore\") \n",
    "                            except Exception:\n",
    "                                attach_text = \"(二进制文件或无法识别编码)\"\n",
    "                    except Exception as e_file:\n",
    "                        attach_text = f\"(附件处理错误: {e_file})\"\n",
    "                    finally:\n",
    "                        if temp_file_path and pathlib.Path(temp_file_path).exists():\n",
    "                            pathlib.Path(temp_file_path).unlink()\n",
    "                \n",
    "                if fn is None: \n",
    "                    prompt_context = f\"【邮件正文】\\n{body}\\n\\n\"\n",
    "                else: \n",
    "                    prompt_context = f\"【邮件正文】\\n{body}\\n\\n【附件: {fn}】\\n{attach_text}\"\n",
    "\n",
    "                prompt = (\n",
    "                    f\"邮件主题: {subj}\\n\"\n",
    "                    f\"发件人: {sender_name} <{sender_email}>\\n\\n\"\n",
    "                    f\"{prompt_context}\"\n",
    "                )\n",
    "                \n",
    "                ans = glm(prompt)\n",
    "                parsed = parse_glm(ans)\n",
    "\n",
    "                if parsed:\n",
    "                    print(f\"    ↪ GLM parsed {len(parsed)} row(s) from {source_name}\")\n",
    "                    for item in parsed:\n",
    "                        row = {c: \"\" for c in COLS}\n",
    "                        row.update(item) \n",
    "                        row.update({\n",
    "                            \"原邮件名\": subj,\n",
    "                            \"发件人\": sender_email,\n",
    "                            \"发件机构\": sender_name \n",
    "                        })\n",
    "                        rows.append(row)\n",
    "                else:\n",
    "                    print(f\"    ↪ 0 rows parsed (or parsing failed) from {source_name}\")\n",
    "    \n",
    "    if actual_emails_processed_count == 0:\n",
    "        print(\"\\n👀 No new emails were found and processed in this run.\")\n",
    "        save_current_run_datetime() \n",
    "        return\n",
    "\n",
    "    if not rows:\n",
    "        print(\"\\n👀 Processed new emails, but no NAV data was captured.\")\n",
    "        save_current_run_datetime() \n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    if df.empty:\n",
    "        print(\"\\n👀 No unique NAV data captured after processing and removing duplicates.\")\n",
    "        save_current_run_datetime() \n",
    "        return\n",
    "    \n",
    "    file_exists = pathlib.Path(XLSX).exists()\n",
    "    excel_writer_mode = \"a\" if file_exists else \"w\" \n",
    "    excel_if_sheet_exists = \"replace\" # Always replace if sheet exists, relevant for mode 'a'\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(XLSX, engine=\"openpyxl\", mode=excel_writer_mode, \n",
    "                            if_sheet_exists=excel_if_sheet_exists) as writer:\n",
    "            # If mode='w' or file didn't exist, it creates a new file.\n",
    "            # If mode='a' and sheet exists, it's replaced.\n",
    "            # If mode='a' and sheet doesn't exist, it's added.\n",
    "            df.to_excel(writer, index=False, sheet_name=SHEET, header=True)\n",
    "        print(f\"\\n✅ {len(df)} unique rows written/updated → {XLSX} (Sheet: {SHEET})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ‼️ Error writing to Excel '{XLSX}': {e}.\")\n",
    "        timestamp_fallback = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fallback_xlsx = f\"{pathlib.Path(XLSX).stem}_fallback_{timestamp_fallback}{pathlib.Path(XLSX).suffix}\"\n",
    "        try:\n",
    "            df.to_excel(fallback_xlsx, index=False, sheet_name=SHEET)\n",
    "            print(f\"\\n⚠️ Data saved to fallback file: {fallback_xlsx}\")\n",
    "        except Exception as fe:\n",
    "            print(f\"    ‼️ Error writing to fallback Excel file '{fallback_xlsx}': {fe}.\")\n",
    "            print(f\"    ℹ️ Raw data rows collected ({len(df)}):\")\n",
    "            # Limiting output for very large dataframes\n",
    "            # for record_idx, record in enumerate(df.to_dict('records')):\n",
    "            #     if record_idx < 10: # Print first 10 records\n",
    "            #         print(f\"      {record}\")\n",
    "            #     elif record_idx == 10:\n",
    "            #         print(f\"      ... (and {len(df)-10} more records)\")\n",
    "            #         break\n",
    "\n",
    "\n",
    "    save_current_run_datetime() # Save timestamp after all processing for this run is complete\n",
    "\n",
    "# ─── run ────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Script interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 An unexpected error occurred in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # This message now prints regardless of success, interrupt, or error in main()\n",
    "        print(\"\\n👋 Script execution cycle finished or was terminated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
