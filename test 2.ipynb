{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c6bbd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¬ 5 messages since 27-Apr-2025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] æ­å–œï¼Œæ‰‹æœºå·ç é‚®ç®±å·²å¼€é€š\n",
      "    From: ç½‘æ˜“é‚®ä»¶ä¸­å¿ƒ <mail@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â†ª 0 rows parsed from æ­£æ–‡\n",
      "\n",
      "[2] é‚®ä»¶åŠå…¬ï¼Œå¦‚æ­¤è½»æ¾ï¼æ¬¢è¿æ¥åˆ°ç½‘æ˜“é‚®ç®±ï¼\n",
      "    From: ç½‘æ˜“é‚®ç®±åŠ©æ‰‹ <club@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â†ª 0 rows parsed from æ­£æ–‡\n",
      "\n",
      "[3] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['Book2.xlsx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â†ª 0 rows parsed from Book2.xlsx\n",
      "\n",
      "[4] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (2): ['Book2.xlsx', 'SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls']\n",
      "    â†ª 0 rows parsed from Book2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â†ª GLM parsed 8 row(s) from SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls\n",
      "\n",
      "[5] Sssa test2åŸºé‡‘å‡€å€¼å‡€å€¼\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:38<00:00,  7.74s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â†ª GLM parsed 4 row(s) from SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls\n",
      "\n",
      "âœ… 12 rows written â†’ 2025-05-27 åŸºé‡‘å‡€å€¼.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fund-NAV harvester v0.8\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. IMAP login (163.com, ID handshake)\n",
    "2. For each message:\n",
    "      â€¢ capture subject + sender + full body text\n",
    "      â€¢ capture every attachment (any filename)\n",
    "      â€¢ send âŸ¨subject + body + attachment textâŸ© to GLM-Z1-Flash\n",
    "3. Parse alphanumeric fund codes & write rows â†’  YYYY-MM-DD åŸºé‡‘å‡€å€¼.xlsx\n",
    "\"\"\"\n",
    "\n",
    "import re, json, tempfile, pathlib, datetime, contextlib, io, warnings\n",
    "from imapclient import IMAPClient\n",
    "import pyzmail, pandas as pd, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional, nicer HTML-to-text if bs4 is around\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    def html2text(html:str)->str:\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\")\n",
    "except ImportError:\n",
    "    def html2text(html:str)->str:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", html)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# â”€â”€â”€ creds & endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAP_HOST  = \"imap.163.com\"\n",
    "EMAIL_USER = \"zhanluekehu@163.com\"\n",
    "EMAIL_PWD  = \"DRqdN38whrnCFPGx\"              # 163 æˆæƒç \n",
    "GLM_KEY    = \"afe7583d73c9d3948f60230e79e08151.Z9HPB84mxuC31DeK\"\n",
    "GLM_URL    = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "MODEL      = \"glm-z1-flash\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "TODAY   = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "XLSX    = f\"{TODAY} åŸºé‡‘å‡€å€¼.xlsx\"\n",
    "SHEET   = TODAY\n",
    "COLS    = [\"æ—¥æœŸ\",\"åŸºé‡‘åç§°\",\"åŸºé‡‘ä»£ç \",\"å•ä½å‡€å€¼\",\"ç´¯è®¡å‡€å€¼\",\n",
    "           \"åŸé‚®ä»¶å\",\"å‘ä»¶äºº\",\"å‘ä»¶æœºæ„\"]\n",
    "\n",
    "# allow 2-10 char alphanumeric fund codes\n",
    "RX = re.compile(\n",
    "    r\"(\\d{4}-\\d{2}-\\d{2}).*?\"          # æ—¥æœŸ\n",
    "    r\"([\\u4e00-\\u9fa5\\w]+).*?\"         # åŸºé‡‘åç§°\n",
    "    r\"([A-Za-z0-9]{2,10}).*?\"          # åŸºé‡‘ä»£ç \n",
    "    r\"([\\d.]+).*?\"                     # å•ä½å‡€å€¼\n",
    "    r\"([\\d.]+)\",                       # ç´¯è®¡å‡€å€¼\n",
    "    re.S)\n",
    "\n",
    "# â”€â”€â”€ helper: fetch recent mail â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_mail(days:int=30):\n",
    "    with IMAPClient(IMAP_HOST, ssl=True) as srv:\n",
    "        srv.login(EMAIL_USER, EMAIL_PWD)\n",
    "        try:\n",
    "            srv.id_({\"name\":\"python\",\"version\":\"0.8\",\"vendor\":\"myclient\",\n",
    "                     \"contact\":EMAIL_USER})\n",
    "        except Exception:\n",
    "            pass\n",
    "        srv.select_folder(\"INBOX\")\n",
    "        since = (datetime.date.today()-datetime.timedelta(days=days)\n",
    "                ).strftime(\"%d-%b-%Y\")\n",
    "        ids = srv.search([\"SINCE\", since])\n",
    "        print(f\"ğŸ“¬ {len(ids)} messages since {since}\\n\")\n",
    "        for mid in tqdm(ids, desc=\"Fetching\", unit=\"mail\"):\n",
    "            raw = srv.fetch([mid], [\"RFC822\"])[mid][b\"RFC822\"]\n",
    "            yield pyzmail.PyzMessage.factory(raw)\n",
    "\n",
    "# â”€â”€â”€ helper: full body text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_body(msg):\n",
    "    if msg.text_part:\n",
    "        charset = msg.text_part.charset or \"utf-8\"\n",
    "        return msg.text_part.get_payload().decode(charset, \"ignore\")\n",
    "    if msg.html_part:\n",
    "        charset = msg.html_part.charset or \"utf-8\"\n",
    "        html = msg.html_part.get_payload().decode(charset, \"ignore\")\n",
    "        return html2text(html)\n",
    "    return \"\"\n",
    "\n",
    "# â”€â”€â”€ helper: list attachments (filename, bytes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def list_attachments(msg):\n",
    "    for part in msg.mailparts:\n",
    "        fn = getattr(part, \"filename\", None)\n",
    "        if fn:\n",
    "            yield fn, part.get_payload()\n",
    "\n",
    "# â”€â”€â”€ helper: call GLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def glm(prompt:str)->str:\n",
    "    res = requests.post(\n",
    "        GLM_URL,\n",
    "        json={\n",
    "            \"model\": MODEL,\n",
    "            \"messages\":[\n",
    "                {\"role\":\"system\",\n",
    "                 \"content\":\"è¯·ä»…ç”¨ JSON æ•°ç»„è¿”å›åŸºé‡‘å‡€å€¼æŒ‡æ ‡ï¼ˆæ—¥æœŸã€åŸºé‡‘åç§°ã€åŸºé‡‘ä»£ç ã€å•ä½å‡€å€¼ã€ç´¯è®¡å‡€å€¼ã€å‘ä»¶æœºæ„ï¼‰ã€‚\"},\n",
    "                {\"role\":\"user\",\"content\":prompt}],\n",
    "            \"temperature\":0.2,\n",
    "            \"max_tokens\":32000,\n",
    "            \"stream\":False},\n",
    "        headers={\"Authorization\":f\"Bearer {GLM_KEY}\"},\n",
    "        timeout=300)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def parse_glm(txt:str):\n",
    "    try:\n",
    "        data=json.loads(txt)\n",
    "        return data if isinstance(data,list) else [data]\n",
    "    except Exception:\n",
    "        return [dict(zip(COLS,m)) for m in RX.findall(txt)]\n",
    "\n",
    "# â”€â”€â”€ main workflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    rows=[]; idx=1\n",
    "    for msg in fetch_mail():\n",
    "        sender_name,sender_email = msg.get_addresses(\"from\")[0]\n",
    "        subj   = msg.get_subject()\n",
    "        body   = get_body(msg)\n",
    "        atts   = list(list_attachments(msg))\n",
    "\n",
    "        print(f\"\\n[{idx}] {subj}\\n    From: {sender_name} <{sender_email}>\\n\"\n",
    "              f\"    Attachments ({len(atts)}): {[fn for fn,_ in atts]}\")\n",
    "        idx+=1\n",
    "\n",
    "        # Always process body alone (even if no attachment)\n",
    "        payloads = atts if atts else [(None, b\"\")]\n",
    "\n",
    "        for fn,blob in payloads:\n",
    "            attach_text=\"(æ— é™„ä»¶)\"\n",
    "            if fn:                                   # we have bytes\n",
    "                with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "                    tmp.write(blob); tmp.flush()\n",
    "                    # try excel â†’ csv\n",
    "                    try:\n",
    "                        df = pd.read_excel(tmp.name)  # full sheet(s)\n",
    "                        attach_text = df.to_csv(index=False)\n",
    "                    except Exception:\n",
    "                        try:                          # fallback plain text\n",
    "                            attach_text = blob.decode(\"utf-8\", \"ignore\")\n",
    "                        except Exception:\n",
    "                            attach_text = \"(binary æ–‡ä»¶é¢„è§ˆçœç•¥)\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"é‚®ä»¶ä¸»é¢˜: {subj}\\n\"\n",
    "                f\"å‘ä»¶äºº: {sender_name}\\n\\n\"\n",
    "                f\"ã€é‚®ä»¶æ­£æ–‡ã€‘\\n{body}\\n\\n\"\n",
    "                f\"ã€é™„ä»¶: {fn or 'æ— '}ã€‘\\n{attach_text}\"\n",
    "            )\n",
    "\n",
    "            ans = glm(prompt)\n",
    "            parsed = parse_glm(ans)\n",
    "\n",
    "            if parsed:\n",
    "                print(f\"    â†ª GLM parsed {len(parsed)} row(s) \"\n",
    "                      f\"from {'æ­£æ–‡' if fn is None else fn}\")\n",
    "                for item in parsed:\n",
    "                    row={c:item.get(c,\"\") for c in COLS}\n",
    "                    row.update({\"åŸé‚®ä»¶å\":subj,\n",
    "                                \"å‘ä»¶äºº\":sender_email,\n",
    "                                \"å‘ä»¶æœºæ„\":sender_name})\n",
    "                    rows.append(row)\n",
    "            else:\n",
    "                print(f\"    â†ª 0 rows parsed from \"\n",
    "                      f\"{'æ­£æ–‡' if fn is None else fn}\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"\\nğŸ‘€ Finished â€“ no NAV data captured.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    append_mode = pathlib.Path(XLSX).exists()\n",
    "    writer_kwargs = dict(engine=\"openpyxl\", mode=\"a\" if append_mode else \"w\")\n",
    "    if append_mode:\n",
    "        writer_kwargs[\"if_sheet_exists\"] = \"replace\"\n",
    "\n",
    "    with pd.ExcelWriter(XLSX, **writer_kwargs) as xw:\n",
    "        df.to_excel(xw, index=False, sheet_name=SHEET)\n",
    "\n",
    "    print(f\"\\nâœ… {len(df)} rows written â†’ {XLSX}\")\n",
    "\n",
    "# â”€â”€â”€ run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    with contextlib.suppress(KeyboardInterrupt):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a2a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¬ 5 messages since 2025-04-27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] æ­å–œï¼Œæ‰‹æœºå·ç é‚®ç®±å·²å¼€é€š\n",
      "    From: ç½‘æ˜“é‚®ä»¶ä¸­å¿ƒ <mail@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¶ä¸­çš„å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜å’Œæ­£æ–‡ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•å…³äºåŸºé‡‘å‡€å€¼çš„æ•°æ®ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œæ­å–œï¼Œæ‰‹æœºå·ç é‚®ç®±å·²...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "\n",
      "[2] é‚®ä»¶åŠå…¬ï¼Œå¦‚æ­¤è½»æ¾ï¼æ¬¢è¿æ¥åˆ°ç½‘æ˜“é‚®ç®±ï¼\n",
      "    From: ç½‘æ˜“é‚®ç®±åŠ©æ‰‹ <club@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶çš„ä¿¡æ¯ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œé‚®ä»¶åŠå…¬ï¼Œå¦‚æ­¤è½»æ¾ï¼æ¬¢è¿æ¥åˆ°ç½‘æ˜“é‚®ç®±ï¼â€ï¼Œçœ‹èµ·æ¥...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "\n",
      "[3] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['Book2.xlsx']\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªé‚®ä»¶ä¸»é¢˜å’Œæ­£æ–‡ï¼Œè¿˜æœ‰é™„ä»¶ï¼Œä½†çœ‹èµ·æ¥æ­£æ–‡å†…å®¹æ˜¯â€œsasasaswtestâ€ï¼Œä¸»é¢˜æ˜¯â€œSssa...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶å†…å®¹ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ç›¸å…³çš„åŸºé‡‘å‡€å€¼æ•°æ®ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa test2...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from Book2.xlsx\n",
      "\n",
      "[4] Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (2): ['Book2.xlsx', 'SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls']\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»ç”¨æˆ·çš„è¦æ±‚ï¼Œç¡®ä¿å®Œå…¨ç†è§£ä»»åŠ¡ã€‚ç”¨æˆ·å¸Œæœ›ä»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ä¸­è¯†åˆ«å‡ºåŸºé‡‘å‡€å€¼æ•°æ®ï¼Œå¹¶ä»¥ç‰¹å®šçš„...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘éœ€è¦ä»”ç»†åˆ†æç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…³äºå…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·è¦æ±‚ä»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ä¸­è¯†åˆ«ç›¸å…³æ•°æ®ï¼Œå¹¶ä»¥JSONæ•°ç»„å½¢å¼è¿”å›ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«æ—¥æœŸã€åŸºé‡‘åç§°ã€...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from Book2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹Ÿæˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œæˆ‘éœ€è¦ä»”ç»†åˆ†æé‚®ä»¶çš„å„ä¸ªéƒ¨åˆ†ï¼Œç¡®ä¿æ­£ç¡®æå–æ•°æ®ã€‚\n",
      "\n",
      "é¦–å…ˆçœ‹é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa te...'\n",
      "    â†ª GLM parsed 1 row(s) from SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls\n",
      "\n",
      "[5] Sssa test2åŸºé‡‘å‡€å€¼å‡€å€¼\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls']\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªé‚®ä»¶ä¸»é¢˜å’Œæ­£æ–‡ï¼Œè¿˜æœ‰é™„ä»¶ï¼Œä½†çœ‹èµ·æ¥æ­£æ–‡å†…å®¹æ˜¯â€œsasasaswtestâ€ï¼Œçœ‹èµ·æ¥å¯èƒ½æ²¡æœ‰å®é™…...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:34<00:00,  6.86s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹Ÿæˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶çš„ä¿¡æ¯ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa test2åŸºé‡‘å‡€å€¼å‡€å€¼â€ï¼Œçœ‹èµ·æ¥å¯èƒ½åŒ…å«åŸº...'\n",
      "    â†ª GLM parsed 1 row(s) from SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls\n",
      "\n",
      "âœ… 2 unique rows written â†’ 2025-05-27 åŸºé‡‘å‡€å€¼.xlsx (Sheet: 2025-05-27)\n",
      "\n",
      "ğŸ‘‹ Script finished or interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fund-NAV harvester v0.9.2 (LLM-focused, improved parsing & prompt)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. IMAP login (163.com, ID handshake)\n",
    "2. For each message:\n",
    "      â€¢ capture subject + sender + full body text\n",
    "      â€¢ capture every attachment (any filename)\n",
    "      â€¢ send âŸ¨subject + body + attachment textâŸ© to GLM-Z1-Flash\n",
    "3. Parse LLM's JSON response & write rows â†’  YYYY-MM-DD åŸºé‡‘å‡€å€¼.xlsx\n",
    "\"\"\"\n",
    "\n",
    "import re, json, tempfile, pathlib, datetime, contextlib, io, warnings\n",
    "from imapclient import IMAPClient\n",
    "import pyzmail, pandas as pd, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional, nicer HTML-to-text if bs4 is around\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    def html2text(html:str)->str:\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\")\n",
    "except ImportError:\n",
    "    def html2text(html:str)->str:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", html)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# â”€â”€â”€ creds & endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAP_HOST  = \"imap.163.com\"\n",
    "EMAIL_USER = \"zhanluekehu@163.com\" # Replace with your actual email\n",
    "EMAIL_PWD  = \"DRqdN38whrnCFPGx\"    # Replace with your actual 163 App Authorization Code\n",
    "GLM_KEY    = \"afe7583d73c9d3948f60230e79e08151.Z9HPB84mxuC31DeK\" # Replace with your actual GLM API Key\n",
    "GLM_URL    = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "MODEL      = \"glm-z1-flash\" # Or your preferred model like \"glm-4\", \"glm-3-turbo\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "TODAY   = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "XLSX    = f\"{TODAY} åŸºé‡‘å‡€å€¼.xlsx\"\n",
    "SHEET   = TODAY\n",
    "COLS    = [\"æ—¥æœŸ\",\"åŸºé‡‘åç§°\",\"åŸºé‡‘ä»£ç \",\"å•ä½å‡€å€¼\",\"ç´¯è®¡å‡€å€¼\",\n",
    "           \"åŸé‚®ä»¶å\",\"å‘ä»¶äºº\",\"å‘ä»¶æœºæ„\"]\n",
    "\n",
    "# â”€â”€â”€ helper: fetch recent mail â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_mail(days:int=30):\n",
    "    with IMAPClient(IMAP_HOST, ssl=True) as srv:\n",
    "        srv.login(EMAIL_USER, EMAIL_PWD)\n",
    "        try:\n",
    "            srv.id_({\"name\":\"python\",\"version\":\"0.9.2\",\"vendor\":\"myclient\",\n",
    "                     \"contact\":EMAIL_USER})\n",
    "        except Exception:\n",
    "            pass \n",
    "        srv.select_folder(\"INBOX\")\n",
    "        since_date = (datetime.date.today()-datetime.timedelta(days=days))\n",
    "        search_criteria = [\"SINCE\", since_date.strftime(\"%d-%b-%Y\")]\n",
    "        \n",
    "        ids = srv.search(search_criteria)\n",
    "        print(f\"ğŸ“¬ {len(ids)} messages since {since_date.strftime('%Y-%m-%d')}\\n\")\n",
    "        if not ids:\n",
    "            return\n",
    "\n",
    "        for mid in tqdm(ids, desc=\"Fetching\", unit=\"mail\"):\n",
    "            raw_email_data = srv.fetch([mid], [\"RFC822\"])[mid]\n",
    "            if b\"RFC822\" in raw_email_data:\n",
    "                yield pyzmail.PyzMessage.factory(raw_email_data[b\"RFC822\"])\n",
    "            else:\n",
    "                print(f\"Warning: Could not fetch RFC822 data for message ID {mid}\")\n",
    "\n",
    "# â”€â”€â”€ helper: full body text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_body(msg):\n",
    "    if msg.text_part:\n",
    "        charset = msg.text_part.charset or \"utf-8\"\n",
    "        return msg.text_part.get_payload().decode(charset, \"ignore\")\n",
    "    if msg.html_part:\n",
    "        charset = msg.html_part.charset or \"utf-8\"\n",
    "        html = msg.html_part.get_payload().decode(charset, \"ignore\")\n",
    "        return html2text(html)\n",
    "    return \"\"\n",
    "\n",
    "# â”€â”€â”€ helper: list attachments (filename, bytes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def list_attachments(msg):\n",
    "    for part in msg.mailparts:\n",
    "        fn = getattr(part, \"filename\", None)\n",
    "        if fn:\n",
    "            payload_bytes = part.get_payload()\n",
    "            if not isinstance(payload_bytes, bytes):\n",
    "                charset = part.charset or \"utf-8\"\n",
    "                try:\n",
    "                    payload_bytes = str(payload_bytes).encode(charset, \"ignore\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ Could not encode attachment '{fn}' payload to bytes: {e}. Skipping.\")\n",
    "                    continue\n",
    "            yield fn, payload_bytes\n",
    "\n",
    "# â”€â”€â”€ helper: call GLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def glm(prompt:str)->str:\n",
    "    system_prompt = \"\"\"æ‚¨æ˜¯ä¸€ä½æå–é‡‘èæ•°æ®çš„ä¸“å®¶ã€‚è¯·ä»æä¾›çš„æ–‡æœ¬ï¼ˆé‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ï¼‰ä¸­è¯†åˆ«å¹¶æå–å…³äºå…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚\n",
    "è¯·å°†ä¿¡æ¯ä»¥ JSON å¯¹è±¡æ•°ç»„çš„å½¢å¼è¿”å›ã€‚æ¯ä¸ªå¯¹è±¡åº”ä»£è¡¨ä¸€åªç‹¬ç«‹çš„åŸºé‡‘ï¼Œå¹¶ç²¾ç¡®åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n",
    "- \"æ—¥æœŸ\": åŸºé‡‘å‡€å€¼çš„æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œæ¥æºäºæ–‡æœ¬å†…å®¹ã€‚\n",
    "- \"åŸºé‡‘åç§°\": åŸºé‡‘çš„åç§°ã€‚\n",
    "- \"åŸºé‡‘ä»£ç \": åŸºé‡‘çš„å­—æ¯æˆ–æ•°å­—ä»£ç ã€‚\n",
    "- \"å•ä½å‡€å€¼\": å•ä½å‡€å€¼ï¼Œåº”ä¸ºä¸€ä¸ªæ•°å­—ã€‚\n",
    "- \"ç´¯è®¡å‡€å€¼\": ç´¯è®¡å‡€å€¼ï¼Œåº”ä¸ºä¸€ä¸ªæ•°å­—ã€‚\n",
    "\n",
    "é‡è¦æç¤ºï¼š\n",
    "- ä»…åŒ…å«æ˜ç¡®çš„åŸºé‡‘å‡€å€¼æ•°æ®æ¡ç›®ã€‚\n",
    "- å¦‚æœåˆ—å‡ºäº†å¤šåªåŸºé‡‘ï¼Œè¯·ä¸ºæ¯åªåŸºé‡‘åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ JSON å¯¹è±¡ã€‚\n",
    "- å¦‚æœåœ¨æ–‡æœ¬ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„åŸºé‡‘å‡€å€¼æ•°æ®ï¼Œè¯·è¿”å›ä¸€ä¸ªç©ºçš„ JSON æ•°ç»„ï¼š[]ã€‚\n",
    "- **æ‚¨çš„å›å¤å¿…é¡»ä¸¥æ ¼éµå®ˆè¾“å‡ºæ ¼å¼ã€‚æ‚¨çš„å›å¤åªèƒ½åŒ…å«ä¸€ä¸ª JSON å¯¹è±¡æ•°ç»„ï¼Œä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡Šã€æ³¨é‡Šæˆ–æ€è€ƒè¿‡ç¨‹ã€‚ç»å¯¹ä¸è¦ä½¿ç”¨ `<think>` æˆ–ä»»ä½•ç±»ä¼¼çš„æ ‡ç­¾ã€‚å¦‚æœæ‰¾ä¸åˆ°æ•°æ®ï¼Œè¯·è¿”å›ç©ºçš„ JSON æ•°ç»„ `[]`ã€‚ä»»ä½•åç¦»æ­¤ JSON-only æ ¼å¼çš„è¾“å‡ºéƒ½å°†è¢«è§†ä¸ºå¤±è´¥ã€‚**\n",
    "- ç¡®ä¿â€œå•ä½å‡€å€¼â€å’Œâ€œç´¯è®¡å‡€å€¼â€çš„å€¼æ˜¯æ•°å­—ã€‚\n",
    "- è¯·ä»”ç»†å‡†ç¡®è¯†åˆ«åŸºé‡‘åç§°å’Œä»£ç ï¼Œé¿å…æå–é€šç”¨æ–‡æœ¬æˆ–æ–‡ä»¶åã€‚\n",
    "- â€œæ—¥æœŸâ€åº”è¯¥æ˜¯ä¸å‡€å€¼ç›¸å…³çš„ç‰¹å®šæ—¥æœŸï¼Œé™¤éæ˜ç¡®è¯´æ˜æ˜¯å‡€å€¼æ—¥æœŸï¼Œå¦åˆ™ä¸ä¸€å®šæ˜¯é‚®ä»¶æ—¥æœŸæˆ–æŠ¥å‘Šç”Ÿæˆæ—¥æœŸã€‚\n",
    "\n",
    "æœŸæœ›çš„å•ä¸ªåŸºé‡‘è¾“å‡ºç¤ºä¾‹ï¼š\n",
    "[\n",
    "  {\n",
    "    \"æ—¥æœŸ\": \"2025-05-26\",\n",
    "    \"åŸºé‡‘åç§°\": \"ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘\",\n",
    "    \"åŸºé‡‘ä»£ç \": \"SQD546\",\n",
    "    \"å•ä½å‡€å€¼\": 1.0580,\n",
    "    \"ç´¯è®¡å‡€å€¼\": 1.5053\n",
    "  }\n",
    "]\n",
    "æ— æ•°æ®æ—¶è¾“å‡ºç¤ºä¾‹ï¼š\n",
    "[]\n",
    "\"\"\"\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            GLM_URL,\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\":[\n",
    "                    {\"role\":\"system\", \"content\": system_prompt},\n",
    "                    {\"role\":\"user\",\"content\":prompt}],\n",
    "                \"temperature\":0.2,\n",
    "                \"max_tokens\":32000,\n",
    "                \"stream\":False},\n",
    "            headers={\"Authorization\":f\"Bearer {GLM_KEY}\"},\n",
    "            timeout=300)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"    â€¼ï¸ GLM API request failed: {e}\")\n",
    "        return \"[]\" \n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        response_text = res.text if 'res' in locals() else \"N/A (response object not available)\"\n",
    "        print(f\"    â€¼ï¸ GLM API response format unexpected or not valid JSON: {e} - Response: {response_text[:200]}\")\n",
    "        return \"[]\"\n",
    "\n",
    "def parse_glm(txt:str):\n",
    "    try:\n",
    "        cleaned_txt = txt.strip()\n",
    "\n",
    "        json_start_index = -1\n",
    "        first_brace = cleaned_txt.find('{')\n",
    "        first_bracket = cleaned_txt.find('[')\n",
    "\n",
    "        if first_brace != -1 and first_bracket != -1:\n",
    "            json_start_index = min(first_brace, first_bracket)\n",
    "        elif first_brace != -1:\n",
    "            json_start_index = first_brace\n",
    "        elif first_bracket != -1:\n",
    "            json_start_index = first_bracket\n",
    "        \n",
    "        if json_start_index > 0:\n",
    "            preceding_text = cleaned_txt[:json_start_index]\n",
    "            if \"<think>\" in preceding_text.lower(): \n",
    "                 print(f\"    â„¹ï¸ Stripped preceding LLM thought process/text: '{preceding_text[:100].strip()}...'\")\n",
    "            else:\n",
    "                 print(f\"    â„¹ï¸ Stripped preceding non-JSON text: '{preceding_text[:100].strip()}...'\")\n",
    "            cleaned_txt = cleaned_txt[json_start_index:]\n",
    "        elif json_start_index == -1 :\n",
    "            if \"<think>\" in cleaned_txt.lower() :\n",
    "                 print(f\"    âš ï¸ GLM output appears to be only thought process/text without JSON: '{cleaned_txt[:200].strip()}...'\")\n",
    "            else:\n",
    "                 print(f\"    âš ï¸ GLM output does not contain valid JSON start character ([ or {{): '{cleaned_txt[:200].strip()}...'\")\n",
    "            return []\n",
    "\n",
    "        if cleaned_txt.startswith(\"```json\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```json\"):].strip()\n",
    "        elif cleaned_txt.startswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```\"):].strip()\n",
    "        if cleaned_txt.endswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[:-len(\"```\")].strip()\n",
    "\n",
    "        if not cleaned_txt:\n",
    "            return []\n",
    "        \n",
    "        data = json.loads(cleaned_txt)\n",
    "        \n",
    "        parsed_items = []\n",
    "        expected_keys = {\"æ—¥æœŸ\", \"åŸºé‡‘åç§°\", \"åŸºé‡‘ä»£ç \", \"å•ä½å‡€å€¼\", \"ç´¯è®¡å‡€å€¼\"}\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and expected_keys.issubset(item.keys()):\n",
    "                    try:\n",
    "                        item[\"å•ä½å‡€å€¼\"] = float(str(item[\"å•ä½å‡€å€¼\"]).replace(',',''))\n",
    "                        item[\"ç´¯è®¡å‡€å€¼\"] = float(str(item[\"ç´¯è®¡å‡€å€¼\"]).replace(',',''))\n",
    "                        parsed_items.append(item)\n",
    "                    except (ValueError, TypeError):\n",
    "                        print(f\"    âš ï¸ GLM list item skipped (net values not convertible to float): {str(item)[:100]}\")\n",
    "                elif isinstance(item, dict):\n",
    "                     print(f\"    âš ï¸ GLM list item skipped (missing expected keys): {str(item)[:100]}\")\n",
    "                else:\n",
    "                    print(f\"    âš ï¸ GLM list item skipped (not a dictionary): {str(item)[:100]}\")\n",
    "            return parsed_items\n",
    "        elif isinstance(data, dict): \n",
    "            if expected_keys.issubset(data.keys()):\n",
    "                try:\n",
    "                    data[\"å•ä½å‡€å€¼\"] = float(str(data[\"å•ä½å‡€å€¼\"]).replace(',',''))\n",
    "                    data[\"ç´¯è®¡å‡€å€¼\"] = float(str(data[\"ç´¯è®¡å‡€å€¼\"]).replace(',',''))\n",
    "                    return [data] \n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"    âš ï¸ GLM dict item skipped (net values not convertible to float): {str(data)[:100]}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"    âš ï¸ GLM dict skipped (missing expected keys): {str(data)[:100]}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"    âš ï¸ GLM output (after stripping) is valid JSON but not a list or dict: {cleaned_txt[:200]}\")\n",
    "            return []\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"    âš ï¸ GLM output (after stripping) was not valid JSON. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ Unexpected error parsing GLM output: {e}. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "\n",
    "# â”€â”€â”€ main workflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    rows=[]\n",
    "    mail_fetch_iterator = fetch_mail()\n",
    "    if mail_fetch_iterator is None:\n",
    "        print(\"\\nğŸ‘€ No messages found based on search criteria.\")\n",
    "        return\n",
    "\n",
    "    for idx, msg in enumerate(mail_fetch_iterator, 1):\n",
    "        if msg is None: continue\n",
    "\n",
    "        sender_addresses = msg.get_addresses(\"from\")\n",
    "        if sender_addresses:\n",
    "            sender_name, sender_email = sender_addresses[0]\n",
    "        else:\n",
    "            sender_name, sender_email = \"Unknown Sender\", \"unknown@example.com\"\n",
    "\n",
    "        subj = msg.get_subject() or \"(No Subject)\"\n",
    "        body = get_body(msg)\n",
    "        atts = list(list_attachments(msg))\n",
    "\n",
    "        print(f\"\\n[{idx}] {subj}\\n    From: {sender_name} <{sender_email}>\\n\"\n",
    "              f\"    Attachments ({len(atts)}): {[fn for fn,_ in atts]}\")\n",
    "\n",
    "        payloads_to_process = [(None, b\"\")] \n",
    "        payloads_to_process.extend(atts)\n",
    "\n",
    "        for fn, blob in payloads_to_process:\n",
    "            attach_text = \"(æ— ç›¸å…³æ–‡æœ¬å†…å®¹)\"\n",
    "            source_name = \"æ­£æ–‡\"\n",
    "\n",
    "            if fn: \n",
    "                source_name = fn\n",
    "                temp_file_path = None\n",
    "                try:\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix=pathlib.Path(fn).suffix) as tmp:\n",
    "                        tmp.write(blob)\n",
    "                        temp_file_path = tmp.name\n",
    "                    \n",
    "                    try:\n",
    "                        xls_content = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "                        if isinstance(xls_content, dict):\n",
    "                            combined_df = pd.concat(xls_content.values(), ignore_index=True)\n",
    "                        else:\n",
    "                            combined_df = xls_content\n",
    "                        attach_text = combined_df.to_csv(index=False, header=True)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            attach_text = blob.decode(\"utf-8\", \"ignore\")\n",
    "                        except UnicodeDecodeError:\n",
    "                             attach_text = blob.decode(\"gbk\", \"ignore\") \n",
    "                        except Exception:\n",
    "                            attach_text = \"(äºŒè¿›åˆ¶æ–‡ä»¶æˆ–æ— æ³•è¯†åˆ«ç¼–ç )\"\n",
    "                except Exception as e_file:\n",
    "                    attach_text = f\"(é™„ä»¶å¤„ç†é”™è¯¯: {e_file})\"\n",
    "                finally:\n",
    "                    if temp_file_path and pathlib.Path(temp_file_path).exists():\n",
    "                        pathlib.Path(temp_file_path).unlink()\n",
    "            \n",
    "            if fn is None: \n",
    "                prompt_context = f\"ã€é‚®ä»¶æ­£æ–‡ã€‘\\n{body}\\n\\n\"\n",
    "            else: \n",
    "                prompt_context = f\"ã€é‚®ä»¶æ­£æ–‡ã€‘\\n{body}\\n\\nã€é™„ä»¶: {fn}ã€‘\\n{attach_text}\"\n",
    "\n",
    "            prompt = (\n",
    "                f\"é‚®ä»¶ä¸»é¢˜: {subj}\\n\"\n",
    "                f\"å‘ä»¶äºº: {sender_name} <{sender_email}>\\n\\n\"\n",
    "                f\"{prompt_context}\"\n",
    "            )\n",
    "            \n",
    "            ans = glm(prompt)\n",
    "            parsed = parse_glm(ans)\n",
    "\n",
    "            if parsed:\n",
    "                print(f\"    â†ª GLM parsed {len(parsed)} row(s) from {source_name}\")\n",
    "                for item in parsed:\n",
    "                    row = {c: \"\" for c in COLS}\n",
    "                    row.update(item) \n",
    "                    row.update({\n",
    "                        \"åŸé‚®ä»¶å\": subj,\n",
    "                        \"å‘ä»¶äºº\": sender_email,\n",
    "                        \"å‘ä»¶æœºæ„\": sender_name\n",
    "                    })\n",
    "                    rows.append(row)\n",
    "            else:\n",
    "                print(f\"    â†ª 0 rows parsed (or parsing failed) from {source_name}\")\n",
    "\n",
    "    if not rows:\n",
    "        print(\"\\nğŸ‘€ Finished â€“ no NAV data captured.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    if df.empty:\n",
    "        print(\"\\nğŸ‘€ No unique NAV data captured after processing and removing duplicates.\")\n",
    "        return\n",
    "    \n",
    "    file_exists = pathlib.Path(XLSX).exists()\n",
    "    excel_writer_mode = \"a\" if file_exists else \"w\"\n",
    "    excel_if_sheet_exists = \"replace\" if file_exists else None\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(XLSX, engine=\"openpyxl\", mode=excel_writer_mode, \n",
    "                            if_sheet_exists=excel_if_sheet_exists) as writer:\n",
    "            df.to_excel(writer, index=False, sheet_name=SHEET, header=True)\n",
    "        print(f\"\\nâœ… {len(df)} unique rows written â†’ {XLSX} (Sheet: {SHEET})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â€¼ï¸ Error writing to Excel '{XLSX}': {e}.\")\n",
    "        fallback_xlsx = f\"{pathlib.Path(XLSX).stem}_fallback{pathlib.Path(XLSX).suffix}\"\n",
    "        try:\n",
    "            df.to_excel(fallback_xlsx, index=False, sheet_name=SHEET)\n",
    "            print(f\"\\nâš ï¸ Data saved to fallback file: {fallback_xlsx}\")\n",
    "        except Exception as fe:\n",
    "            print(f\"    â€¼ï¸ Error writing to fallback Excel file '{fallback_xlsx}': {fe}.\")\n",
    "\n",
    "# â”€â”€â”€ run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    with contextlib.suppress(KeyboardInterrupt):\n",
    "        main()\n",
    "    print(\"\\nğŸ‘‹ Script finished or interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01f8e64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Last run timestamp file not found. Processing with default window.\n",
      "ğŸ“¬ Found 7 email candidates from last 30 days (server search from date: 2025-04-27).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:   0%|          | 0/7 [00:00<?, ?mail/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Processing: æ­å–œï¼Œæ‰‹æœºå·ç é‚®ç®±å·²å¼€é€š\n",
      "    From: ç½‘æ˜“é‚®ä»¶ä¸­å¿ƒ <mail@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  14%|â–ˆâ–        | 1/7 [00:02<00:14,  2.36s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œå¹¶ä»ä¸­æå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜å’Œæ­£æ–‡ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä»»ä½•å…³äºåŸºé‡‘å‡€å€¼çš„æ•°æ®ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œæ­å–œï¼Œæ‰‹æœºå·ç é‚®ç®±å·²...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "\n",
      "[2] Processing: é‚®ä»¶åŠå…¬ï¼Œå¦‚æ­¤è½»æ¾ï¼æ¬¢è¿æ¥åˆ°ç½‘æ˜“é‚®ç®±ï¼\n",
      "    From: ç½‘æ˜“é‚®ç®±åŠ©æ‰‹ <club@service.netease.com>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:05<00:15,  3.08s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¶ä¸­çš„å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»ç”¨æˆ·çš„è¦æ±‚ï¼Œç¡®ä¿å®Œå…¨ç†è§£ä»»åŠ¡ã€‚ç”¨æˆ·å¸Œæœ›ä»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ä¸­è¯†åˆ«å‡ºåŸºé‡‘å‡€å€¼æ•°æ®ï¼Œå¹¶ä»¥...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "\n",
      "[3] Processing: Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['Book2.xlsx']\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªé‚®ä»¶ä¸»é¢˜å’Œæ­£æ–‡ï¼Œè¿˜æœ‰é™„ä»¶ï¼Œä½†çœ‹èµ·æ¥æ­£æ–‡å†…å®¹æ˜¯â€œsasasaswtestâ€ï¼Œä¸»é¢˜æ˜¯â€œSssa...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3/7 [00:12<00:18,  4.71s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œä»ä¸­æå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶å†…å®¹ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ç›¸å…³çš„åŸºé‡‘å‡€å€¼æ•°æ®ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa tes...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from Book2.xlsx\n",
      "\n",
      "[4] Processing: Sssa test2\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (2): ['Book2.xlsx', 'SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls']\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»ç”¨æˆ·çš„è¦æ±‚ï¼Œç¡®ä¿å®Œå…¨ç†è§£ä»»åŠ¡ã€‚ç”¨æˆ·å¸Œæœ›ä»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ä¸­è¯†åˆ«å‡ºåŸºé‡‘å‡€å€¼æ•°æ®ï¼Œå¹¶ä»¥ä¸¥æ ¼çš„...'\n",
      "    âš ï¸ GLM output (after stripping) was not valid JSON. Original start: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»ç”¨æˆ·çš„è¦æ±‚ï¼Œç¡®ä¿å®Œå…¨ç†è§£ä»»åŠ¡ã€‚ç”¨æˆ·å¸Œæœ›ä»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ä¸­è¯†åˆ«å‡ºåŸºé‡‘å‡€å€¼æ•°æ®ï¼Œå¹¶ä»¥ä¸¥æ ¼çš„...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œä»ä¸­æå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶çš„å†…å®¹ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ç›¸å…³çš„åŸºé‡‘å‡€å€¼æ•°æ®ã€‚\n",
      "\n",
      "é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa te...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from Book2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:25<00:23,  7.80s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹Ÿæˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·è¦æ±‚ä»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ä¸­è¯†åˆ«ç›¸å…³æ•°æ®ï¼Œå¹¶æŒ‰ç…§ä¸¥æ ¼çš„JSONæ ¼å¼è¿”å›ã€‚\n",
      "\n",
      "é¦–å…ˆçœ‹é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa...'\n",
      "    â†ª GLM parsed 1 row(s) from SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls\n",
      "\n",
      "[5] Processing: Sssa test2åŸºé‡‘å‡€å€¼å‡€å€¼\n",
      "    From: Chengyi Xu <chengyi_xu@outlook.com>\n",
      "    Attachments (1): ['SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls']\n",
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªé‚®ä»¶ä¸»é¢˜å’Œæ­£æ–‡ï¼Œè¿˜æœ‰é™„ä»¶ï¼Œä½†çœ‹èµ·æ¥æ­£æ–‡å†…å®¹æ˜¯â€œsasasaswtestâ€ï¼Œä¸»é¢˜æ˜¯â€œSssa...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:32<00:15,  7.54s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹Ÿæˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·ç»™äº†ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œæˆ‘éœ€è¦ä¸¥æ ¼æŒ‰ç…§ä»–ä»¬çš„è¦æ±‚æ¥æ“ä½œã€‚\n",
      "\n",
      "é¦–å…ˆçœ‹é‚®ä»¶ä¸»é¢˜æ˜¯â€œSssa test2åŸºé‡‘å‡€å€¼å‡€...'\n",
      "    â†ª GLM parsed 1 row(s) from SQD546_ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘_2025-05-26_å‡€å€¼è¡¨.xls\n",
      "\n",
      "[6] Processing: åŠå…¬å®¤å…±äº«äº†æ–‡ä»¶ç»™æ‚¨\n",
      "    From: svyxvl@sjwdif8f.cn <svyxvl@sjwdif8f.cn>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:34<00:05,  5.78s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†ç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…¬å‹Ÿæˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘è¦ä»”ç»†é˜…è¯»é‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶çš„ä¿¡æ¯ã€‚é‚®ä»¶ä¸»é¢˜æ˜¯â€œåŠå…¬å®¤å…±äº«äº†æ–‡ä»¶ç»™æ‚¨â€ï¼Œå‘ä»¶äººåœ°å€çœ‹èµ·æ¥æœ‰ç‚¹å¥‡æ€ªï¼Œå¯èƒ½æ˜¯...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "\n",
      "[7] Processing: ã€å›½ä¿¡æ‰˜ç®¡ã€‘æ‚¨ç®¡ç†çš„ä»¥ä¸‹äº§å“äº2025å¹´05æœˆ27æ—¥ ä¸´æ—¶å¼€æ”¾ï¼Œçƒ¦è¯·æ‚¨å‘å…¨ä½“æŠ•èµ„äººæŠ«éœ²è¯¥ä¸´æ—¶å¼€æ”¾æ—¥äº‹é¡¹\n",
      "    From: gxtggzhs@guosen.com.cn <gxtggzhs@guosen.com.cn>\n",
      "    Attachments (0): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching & Filtering: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.31s/mail]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    â„¹ï¸ Stripped preceding LLM thought process/text: '<think>\n",
      "å¥½çš„ï¼Œæˆ‘éœ€è¦ä»”ç»†åˆ†æç”¨æˆ·æä¾›çš„é‚®ä»¶å†…å®¹ï¼Œæå–å…³äºå…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚é¦–å…ˆï¼Œç”¨æˆ·è¦æ±‚åªæå–æ˜ç¡®çš„å‡€å€¼æ•°æ®ï¼Œå¹¶ä¸”æ¯ä¸ªåŸºé‡‘è¦å•ç‹¬ä½œä¸ºä¸€ä¸ªJSONå¯¹è±¡ã€‚å¦‚æœæ²¡æ‰¾åˆ°æ•°æ®ï¼Œå°±è¿”å›ç©ºæ•°ç»„ã€‚...'\n",
      "    â†ª 0 rows parsed (or parsing failed) from æ­£æ–‡\n",
      "\n",
      "âœ… 2 unique rows written/updated â†’ 2025-05-27 åŸºé‡‘å‡€å€¼.xlsx (Sheet: 2025-05-27)\n",
      "â˜‘ï¸ Saved current run timestamp: 2025-05-27 15:25:20 UTC to log/last_run.txt\n",
      "\n",
      "ğŸ‘‹ Script execution cycle finished or was terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Fund-NAV harvester v0.9.3 (LLM-focused, incremental processing, improved parsing & prompt)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. IMAP login (163.com, ID handshake)\n",
    "2. Read last run timestamp.\n",
    "3. For each new message (since last run):\n",
    "     â€¢ capture subject + sender + full body text\n",
    "     â€¢ capture every attachment (any filename)\n",
    "     â€¢ send âŸ¨subject + body + attachment textâŸ© to GLM-Z1-Flash\n",
    "4. Parse LLM's JSON response & write rows â†’ YYYY-MM-DD åŸºé‡‘å‡€å€¼.xlsx\n",
    "5. Save current run timestamp.\n",
    "\"\"\"\n",
    "\n",
    "import re, json, tempfile, pathlib, datetime, contextlib, io, warnings\n",
    "from imapclient import IMAPClient\n",
    "import pyzmail, pandas as pd, requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optional, nicer HTML-to-text if bs4 is around\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "    def html2text(html:str)->str:\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text(\"\\n\")\n",
    "except ImportError:\n",
    "    def html2text(html:str)->str:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", html)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# â”€â”€â”€ creds & endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAP_HOST  = \"imap.163.com\"\n",
    "EMAIL_USER = \"zhanluekehu@163.com\" # Replace with your actual email\n",
    "EMAIL_PWD  = \"DRqdN38whrnCFPGx\"    # Replace with your actual 163 App Authorization Code\n",
    "GLM_KEY    = \"afe7583d73c9d3948f60230e79e08151.Z9HPB84mxuC31DeK\" # Replace with your actual GLM API Key\n",
    "GLM_URL    = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n",
    "MODEL      = \"glm-z1-flash\" # Or your preferred model like \"glm-4\", \"glm-3-turbo\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "TODAY   = datetime.date.today().strftime(\"%Y-%m-%d\") # Used for default Excel sheet name\n",
    "XLSX    = f\"{TODAY} åŸºé‡‘å‡€å€¼.xlsx\" # Output Excel filename uses current date\n",
    "SHEET   = TODAY # Sheet name is current date\n",
    "COLS    = [\"æ—¥æœŸ\",\"åŸºé‡‘åç§°\",\"åŸºé‡‘ä»£ç \",\"å•ä½å‡€å€¼\",\"ç´¯è®¡å‡€å€¼\",\n",
    "           \"åŸé‚®ä»¶å\",\"å‘ä»¶äºº\",\"å‘ä»¶æœºæ„\"]\n",
    "\n",
    "# â”€â”€â”€ Timestamp logging for incremental processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LOG_DIR = pathlib.Path(\"log\")\n",
    "LAST_RUN_FILE = LOG_DIR / \"last_run.txt\"\n",
    "DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\" # UTC datetime format for the log file\n",
    "\n",
    "def get_last_run_datetime() -> datetime.datetime | None:\n",
    "    \"\"\"Reads the last successful run datetime from the log file (expects UTC).\"\"\"\n",
    "    if not LAST_RUN_FILE.exists():\n",
    "        print(\"â„¹ï¸ Last run timestamp file not found. Processing with default window.\")\n",
    "        return None\n",
    "    try:\n",
    "        content = LAST_RUN_FILE.read_text().strip()\n",
    "        if not content:\n",
    "            print(\"â„¹ï¸ Last run timestamp file is empty. Processing with default window.\")\n",
    "            return None\n",
    "        dt_naive = datetime.datetime.strptime(content, DATETIME_FORMAT)\n",
    "        # Assume stored time is UTC, make it timezone-aware\n",
    "        dt_utc = dt_naive.replace(tzinfo=datetime.timezone.utc)\n",
    "        print(f\"â„¹ï¸ Previous run timestamp: {dt_utc.strftime(DATETIME_FORMAT)} UTC\")\n",
    "        return dt_utc\n",
    "    except (ValueError, OSError) as e:\n",
    "        print(f\"âš ï¸ Error reading or parsing last run timestamp from {LAST_RUN_FILE}: {e}. Processing with default window.\")\n",
    "        return None\n",
    "\n",
    "def save_current_run_datetime():\n",
    "    \"\"\"Saves the current datetime (UTC) as the last successful run timestamp.\"\"\"\n",
    "    try:\n",
    "        LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        now_utc = datetime.datetime.now(datetime.timezone.utc)\n",
    "        LAST_RUN_FILE.write_text(now_utc.strftime(DATETIME_FORMAT))\n",
    "        print(f\"â˜‘ï¸ Saved current run timestamp: {now_utc.strftime(DATETIME_FORMAT)} UTC to {LAST_RUN_FILE}\")\n",
    "    except OSError as e:\n",
    "        print(f\"âš ï¸ Could not save current run timestamp to {LAST_RUN_FILE}: {e}\")\n",
    "\n",
    "# â”€â”€â”€ helper: fetch mail (modified for incremental processing) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_mail(last_run_utc_dt: datetime.datetime | None = None, default_days_lookback: int = 30):\n",
    "    \"\"\"\n",
    "    Fetches emails. If last_run_utc_dt is provided, fetches emails SINCE that date\n",
    "    and then filters by time. Otherwise, fetches emails from default_days_lookback.\n",
    "    Yields pyzmail.PyzMessage objects.\n",
    "    \"\"\"\n",
    "    with IMAPClient(IMAP_HOST, ssl=True) as srv:\n",
    "        srv.login(EMAIL_USER, EMAIL_PWD)\n",
    "        try:\n",
    "            srv.id_({\"name\":\"python\",\"version\":\"0.9.3\",\"vendor\":\"myclient\", # Updated version\n",
    "                     \"contact\":EMAIL_USER})\n",
    "        except Exception:\n",
    "            pass # Optional, continue if ID command fails\n",
    "        \n",
    "        srv.select_folder(\"INBOX\")\n",
    "        \n",
    "        search_description = \"\"\n",
    "        using_last_run_filter = False\n",
    "\n",
    "        if last_run_utc_dt:\n",
    "            # IMAP SINCE uses date part. Server returns all emails on or after this date.\n",
    "            # Time-based filtering will be done client-side using INTERNALDATE.\n",
    "            # Ensure last_run_utc_dt is UTC-aware for comparison.\n",
    "            if last_run_utc_dt.tzinfo is None or last_run_utc_dt.tzinfo.utcoffset(last_run_utc_dt) is None:\n",
    "                last_run_utc_dt = last_run_utc_dt.replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "            since_date_for_imap = last_run_utc_dt.date()\n",
    "            search_criteria = [\"SINCE\", since_date_for_imap]\n",
    "            search_description = (f\"candidates since {last_run_utc_dt.strftime(DATETIME_FORMAT)} UTC \"\n",
    "                                  f\"(server search from date: {since_date_for_imap.strftime('%Y-%m-%d')})\")\n",
    "            using_last_run_filter = True\n",
    "        else:\n",
    "            # Fallback to default lookback period if no last run timestamp\n",
    "            since_date_for_imap = (datetime.datetime.now(datetime.timezone.utc).date() - \n",
    "                                   datetime.timedelta(days=default_days_lookback))\n",
    "            search_criteria = [\"SINCE\", since_date_for_imap] # imapclient handles date obj\n",
    "            search_description = (f\"candidates from last {default_days_lookback} days \"\n",
    "                                  f\"(server search from date: {since_date_for_imap.strftime('%Y-%m-%d')})\")\n",
    "\n",
    "        ids = srv.search(search_criteria)\n",
    "        print(f\"ğŸ“¬ Found {len(ids)} email {search_description}.\")\n",
    "        \n",
    "        if not ids:\n",
    "            print(\"No emails matched server-side criteria.\\n\")\n",
    "            return\n",
    "\n",
    "        for mid in tqdm(ids, desc=\"Fetching & Filtering\", unit=\"mail\", mininterval=0.5, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'):\n",
    "            raw_email_data_map = srv.fetch([mid], [\"RFC822\", \"INTERNALDATE\"])\n",
    "            \n",
    "            if not raw_email_data_map or mid not in raw_email_data_map:\n",
    "                tqdm.write(f\"Warning: Could not fetch full data for message ID {mid}\")\n",
    "                continue \n",
    "            \n",
    "            message_data = raw_email_data_map[mid]\n",
    "\n",
    "            if b\"RFC822\" not in message_data:\n",
    "                tqdm.write(f\"Warning: Could not fetch RFC822 (body) for message ID {mid}\")\n",
    "                continue\n",
    "\n",
    "            if using_last_run_filter:\n",
    "                internal_date_from_server = message_data.get(b'INTERNALDATE') # datetime obj from imapclient\n",
    "                \n",
    "                if internal_date_from_server:\n",
    "                    if internal_date_from_server.tzinfo is None or \\\n",
    "                       internal_date_from_server.tzinfo.utcoffset(internal_date_from_server) is None:\n",
    "                        internal_date_from_server = internal_date_from_server.replace(tzinfo=datetime.timezone.utc)\n",
    "                    \n",
    "                    if internal_date_from_server <= last_run_utc_dt:\n",
    "                        continue \n",
    "                else:\n",
    "                    tqdm.write(f\"Warning: Message ID {mid} missing INTERNALDATE. Cannot filter by exact time. Processing due to date match.\")\n",
    "\n",
    "            yield pyzmail.PyzMessage.factory(message_data[b\"RFC822\"])\n",
    "\n",
    "# â”€â”€â”€ helper: full body text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_body(msg):\n",
    "    if msg.text_part:\n",
    "        charset = msg.text_part.charset or \"utf-8\"\n",
    "        return msg.text_part.get_payload().decode(charset, \"ignore\")\n",
    "    if msg.html_part:\n",
    "        charset = msg.html_part.charset or \"utf-8\"\n",
    "        html = msg.html_part.get_payload().decode(charset, \"ignore\")\n",
    "        return html2text(html)\n",
    "    return \"\"\n",
    "\n",
    "# â”€â”€â”€ helper: list attachments (filename, bytes) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def list_attachments(msg):\n",
    "    for part in msg.mailparts:\n",
    "        fn = getattr(part, \"filename\", None)\n",
    "        if fn:\n",
    "            payload_bytes = part.get_payload()\n",
    "            if not isinstance(payload_bytes, bytes):\n",
    "                charset = part.charset or \"utf-8\"\n",
    "                try:\n",
    "                    payload_bytes = str(payload_bytes).encode(charset, \"ignore\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ Could not encode attachment '{fn}' payload to bytes: {e}. Skipping.\")\n",
    "                    continue\n",
    "            yield fn, payload_bytes\n",
    "\n",
    "# â”€â”€â”€ helper: call GLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def glm(prompt:str)->str:\n",
    "    system_prompt = \"\"\"æ‚¨æ˜¯ä¸€ä½æå–é‡‘èæ•°æ®çš„ä¸“å®¶ã€‚è¯·ä»æä¾›çš„æ–‡æœ¬ï¼ˆé‚®ä»¶ä¸»é¢˜ã€æ­£æ–‡å’Œé™„ä»¶ï¼‰ä¸­è¯†åˆ«å¹¶æå–å…³äºå…¬å‹ŸåŸºé‡‘æˆ–ç§å‹ŸåŸºé‡‘çš„å‡€å€¼ä¿¡æ¯ã€‚\n",
    "è¯·å°†ä¿¡æ¯ä»¥ JSON å¯¹è±¡æ•°ç»„çš„å½¢å¼è¿”å›ã€‚æ¯ä¸ªå¯¹è±¡åº”ä»£è¡¨ä¸€åªç‹¬ç«‹çš„åŸºé‡‘ï¼Œå¹¶ç²¾ç¡®åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n",
    "- \"æ—¥æœŸ\": åŸºé‡‘å‡€å€¼çš„æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œæ¥æºäºæ–‡æœ¬å†…å®¹ã€‚\n",
    "- \"åŸºé‡‘åç§°\": åŸºé‡‘çš„åç§°ã€‚\n",
    "- \"åŸºé‡‘ä»£ç \": åŸºé‡‘çš„å­—æ¯æ•°å­—ä»£ç ã€‚\n",
    "- \"å•ä½å‡€å€¼\": å•ä½å‡€å€¼ï¼Œåº”ä¸ºä¸€ä¸ªæ•°å­—ã€‚\n",
    "- \"ç´¯è®¡å‡€å€¼\": ç´¯è®¡å‡€å€¼ï¼Œåº”ä¸ºä¸€ä¸ªæ•°å­—ã€‚\n",
    "\n",
    "é‡è¦æç¤ºï¼š\n",
    "- ä»…åŒ…å«æ˜ç¡®çš„åŸºé‡‘å‡€å€¼æ•°æ®æ¡ç›®ã€‚\n",
    "- å¦‚æœåˆ—å‡ºäº†å¤šåªåŸºé‡‘ï¼Œè¯·ä¸ºæ¯åªåŸºé‡‘åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ JSON å¯¹è±¡ã€‚\n",
    "- å¦‚æœåœ¨æ–‡æœ¬ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„åŸºé‡‘å‡€å€¼æ•°æ®ï¼Œè¯·è¿”å›ä¸€ä¸ªç©ºçš„ JSON æ•°ç»„ï¼š[]ã€‚\n",
    "- **æ‚¨çš„å›å¤å¿…é¡»ä¸¥æ ¼éµå®ˆè¾“å‡ºæ ¼å¼ã€‚æ‚¨çš„å›å¤åªèƒ½åŒ…å«ä¸€ä¸ª JSON å¯¹è±¡æ•°ç»„ï¼Œä¸èƒ½æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡Šã€æ³¨é‡Šæˆ–æ€è€ƒè¿‡ç¨‹ã€‚ç»å¯¹ä¸è¦ä½¿ç”¨ `<think>` æˆ–ä»»ä½•ç±»ä¼¼çš„æ ‡ç­¾ã€‚å¦‚æœæ‰¾ä¸åˆ°æ•°æ®ï¼Œè¯·è¿”å›ç©ºçš„ JSON æ•°ç»„ `[]`ã€‚ä»»ä½•åç¦»æ­¤ JSON-only æ ¼å¼çš„è¾“å‡ºéƒ½å°†è¢«è§†ä¸ºå¤±è´¥ã€‚**\n",
    "- ç¡®ä¿â€œå•ä½å‡€å€¼â€å’Œâ€œç´¯è®¡å‡€å€¼â€çš„å€¼æ˜¯æ•°å­—ã€‚\n",
    "- è¯·ä»”ç»†å‡†ç¡®è¯†åˆ«åŸºé‡‘åç§°å’Œä»£ç ï¼Œé¿å…æå–é€šç”¨æ–‡æœ¬æˆ–æ–‡ä»¶åã€‚\n",
    "- â€œæ—¥æœŸâ€åº”è¯¥æ˜¯ä¸å‡€å€¼ç›¸å…³çš„ç‰¹å®šæ—¥æœŸï¼Œé™¤éæ˜ç¡®è¯´æ˜æ˜¯å‡€å€¼æ—¥æœŸï¼Œå¦åˆ™ä¸ä¸€å®šæ˜¯é‚®ä»¶æ—¥æœŸæˆ–æŠ¥å‘Šç”Ÿæˆæ—¥æœŸã€‚\n",
    "\n",
    "æœŸæœ›çš„å•ä¸ªåŸºé‡‘è¾“å‡ºç¤ºä¾‹ï¼š\n",
    "[\n",
    "  {\n",
    "    \"æ—¥æœŸ\": \"2025-05-26\",\n",
    "    \"åŸºé‡‘åç§°\": \"ä¹æ‹›çœŸæ ¼é‡åŒ–å¥—åˆ©ä¸€å·ç§å‹Ÿè¯åˆ¸æŠ•èµ„åŸºé‡‘\",\n",
    "    \"åŸºé‡‘ä»£ç \": \"SQD546\",\n",
    "    \"å•ä½å‡€å€¼\": 1.0580,\n",
    "    \"ç´¯è®¡å‡€å€¼\": 1.5053\n",
    "  }\n",
    "]\n",
    "æ— æ•°æ®æ—¶è¾“å‡ºç¤ºä¾‹ï¼š\n",
    "[]\n",
    "\"\"\"\n",
    "    try:\n",
    "        res = requests.post(\n",
    "            GLM_URL,\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"messages\":[\n",
    "                    {\"role\":\"system\", \"content\": system_prompt},\n",
    "                    {\"role\":\"user\",\"content\":prompt}],\n",
    "                \"temperature\":0.2,\n",
    "                \"max_tokens\":32000, # Increased as per original example\n",
    "                \"stream\":False},\n",
    "            headers={\"Authorization\":f\"Bearer {GLM_KEY}\"},\n",
    "            timeout=300)\n",
    "        res.raise_for_status()\n",
    "        return res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"    â€¼ï¸ GLM API request failed: {e}\")\n",
    "        return \"[]\" \n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        response_text = res.text if 'res' in locals() else \"N/A (response object not available)\"\n",
    "        print(f\"    â€¼ï¸ GLM API response format unexpected or not valid JSON: {e} - Response: {response_text[:200]}\")\n",
    "        return \"[]\"\n",
    "\n",
    "# â”€â”€â”€ helper: parse GLM response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def parse_glm(txt:str):\n",
    "    try:\n",
    "        cleaned_txt = txt.strip()\n",
    "\n",
    "        json_start_index = -1\n",
    "        first_brace = cleaned_txt.find('{')\n",
    "        first_bracket = cleaned_txt.find('[')\n",
    "\n",
    "        if first_brace != -1 and first_bracket != -1:\n",
    "            json_start_index = min(first_brace, first_bracket)\n",
    "        elif first_brace != -1:\n",
    "            json_start_index = first_brace\n",
    "        elif first_bracket != -1:\n",
    "            json_start_index = first_bracket\n",
    "        \n",
    "        if json_start_index > 0:\n",
    "            preceding_text = cleaned_txt[:json_start_index]\n",
    "            if \"<think>\" in preceding_text.lower(): \n",
    "                print(f\"    â„¹ï¸ Stripped preceding LLM thought process/text: '{preceding_text[:100].strip()}...'\")\n",
    "            else:\n",
    "                print(f\"    â„¹ï¸ Stripped preceding non-JSON text: '{preceding_text[:100].strip()}...'\")\n",
    "            cleaned_txt = cleaned_txt[json_start_index:]\n",
    "        elif json_start_index == -1 :\n",
    "            if \"<think>\" in cleaned_txt.lower() :\n",
    "                print(f\"    âš ï¸ GLM output appears to be only thought process/text without JSON: '{cleaned_txt[:200].strip()}...'\")\n",
    "            else:\n",
    "                print(f\"    âš ï¸ GLM output does not contain valid JSON start character ([ or {{): '{cleaned_txt[:200].strip()}...'\")\n",
    "            return []\n",
    "\n",
    "        if cleaned_txt.startswith(\"```json\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```json\"):].strip()\n",
    "        elif cleaned_txt.startswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[len(\"```\"):].strip()\n",
    "        if cleaned_txt.endswith(\"```\"):\n",
    "            cleaned_txt = cleaned_txt[:-len(\"```\")].strip()\n",
    "\n",
    "        if not cleaned_txt:\n",
    "            return []\n",
    "        \n",
    "        data = json.loads(cleaned_txt)\n",
    "        \n",
    "        parsed_items = []\n",
    "        expected_keys = {\"æ—¥æœŸ\", \"åŸºé‡‘åç§°\", \"åŸºé‡‘ä»£ç \", \"å•ä½å‡€å€¼\", \"ç´¯è®¡å‡€å€¼\"}\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict) and expected_keys.issubset(item.keys()):\n",
    "                    try:\n",
    "                        item[\"å•ä½å‡€å€¼\"] = float(str(item[\"å•ä½å‡€å€¼\"]).replace(',',''))\n",
    "                        item[\"ç´¯è®¡å‡€å€¼\"] = float(str(item[\"ç´¯è®¡å‡€å€¼\"]).replace(',',''))\n",
    "                        parsed_items.append(item)\n",
    "                    except (ValueError, TypeError):\n",
    "                        print(f\"    âš ï¸ GLM list item skipped (net values not convertible to float): {str(item)[:100]}\")\n",
    "                elif isinstance(item, dict):\n",
    "                    print(f\"    âš ï¸ GLM list item skipped (missing expected keys): {str(item)[:100]}\")\n",
    "                else:\n",
    "                    print(f\"    âš ï¸ GLM list item skipped (not a dictionary): {str(item)[:100]}\")\n",
    "            return parsed_items\n",
    "        elif isinstance(data, dict): \n",
    "            if expected_keys.issubset(data.keys()):\n",
    "                try:\n",
    "                    data[\"å•ä½å‡€å€¼\"] = float(str(data[\"å•ä½å‡€å€¼\"]).replace(',',''))\n",
    "                    data[\"ç´¯è®¡å‡€å€¼\"] = float(str(data[\"ç´¯è®¡å‡€å€¼\"]).replace(',',''))\n",
    "                    return [data] \n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"    âš ï¸ GLM dict item skipped (net values not convertible to float): {str(data)[:100]}\")\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"    âš ï¸ GLM dict skipped (missing expected keys): {str(data)[:100]}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"    âš ï¸ GLM output (after stripping) is valid JSON but not a list or dict: {cleaned_txt[:200]}\")\n",
    "            return []\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"    âš ï¸ GLM output (after stripping) was not valid JSON. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"    âš ï¸ Unexpected error parsing GLM output: {e}. Original start: '{txt[:100].strip()}...'\")\n",
    "        return []\n",
    "\n",
    "# â”€â”€â”€ main workflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    # Ensure log directory exists (also created by save_current_run_datetime if needed)\n",
    "    LOG_DIR.mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    last_run_dt_utc = get_last_run_datetime()\n",
    "    \n",
    "    rows = []\n",
    "    # Pass the last run datetime to fetch_mail; default_days_lookback is used if last_run_dt_utc is None\n",
    "    mail_fetch_iterator = fetch_mail(last_run_utc_dt=last_run_dt_utc, default_days_lookback=30)\n",
    "    \n",
    "    actual_emails_processed_count = 0\n",
    "    if mail_fetch_iterator:\n",
    "        for loop_idx, msg in enumerate(mail_fetch_iterator, 1):\n",
    "            actual_emails_processed_count = loop_idx \n",
    "            if msg is None: continue\n",
    "\n",
    "            sender_addresses = msg.get_addresses(\"from\")\n",
    "            if sender_addresses:\n",
    "                sender_name, sender_email = sender_addresses[0]\n",
    "            else:\n",
    "                sender_name, sender_email = \"Unknown Sender\", \"unknown@example.com\"\n",
    "\n",
    "            subj = msg.get_subject() or \"(No Subject)\"\n",
    "            body = get_body(msg)\n",
    "            atts = list(list_attachments(msg))\n",
    "\n",
    "            print(f\"\\n[{actual_emails_processed_count}] Processing: {subj}\\n    From: {sender_name} <{sender_email}>\\n\"\n",
    "                  f\"    Attachments ({len(atts)}): {[fn for fn,_ in atts]}\")\n",
    "\n",
    "            payloads_to_process = [(None, b\"\")] \n",
    "            payloads_to_process.extend(atts)\n",
    "\n",
    "            for fn, blob in payloads_to_process:\n",
    "                attach_text = \"(æ— ç›¸å…³æ–‡æœ¬å†…å®¹)\"\n",
    "                source_name = \"æ­£æ–‡\"\n",
    "\n",
    "                if fn: \n",
    "                    source_name = fn\n",
    "                    temp_file_path = None\n",
    "                    try:\n",
    "                        # Create a temporary file with the correct suffix for pandas to infer type\n",
    "                        with tempfile.NamedTemporaryFile(delete=False, suffix=pathlib.Path(fn).suffix) as tmp:\n",
    "                            tmp.write(blob)\n",
    "                            temp_file_path = tmp.name\n",
    "                        \n",
    "                        try:\n",
    "                            xls_content = pd.read_excel(temp_file_path, sheet_name=None)\n",
    "                            if isinstance(xls_content, dict):\n",
    "                                combined_df = pd.concat(xls_content.values(), ignore_index=True)\n",
    "                            else:\n",
    "                                combined_df = xls_content\n",
    "                            attach_text = combined_df.to_csv(index=False, header=True)\n",
    "                        except Exception:\n",
    "                            try:\n",
    "                                attach_text = blob.decode(\"utf-8\", \"ignore\")\n",
    "                            except UnicodeDecodeError:\n",
    "                                attach_text = blob.decode(\"gbk\", \"ignore\") \n",
    "                            except Exception:\n",
    "                                attach_text = \"(äºŒè¿›åˆ¶æ–‡ä»¶æˆ–æ— æ³•è¯†åˆ«ç¼–ç )\"\n",
    "                    except Exception as e_file:\n",
    "                        attach_text = f\"(é™„ä»¶å¤„ç†é”™è¯¯: {e_file})\"\n",
    "                    finally:\n",
    "                        if temp_file_path and pathlib.Path(temp_file_path).exists():\n",
    "                            pathlib.Path(temp_file_path).unlink()\n",
    "                \n",
    "                if fn is None: \n",
    "                    prompt_context = f\"ã€é‚®ä»¶æ­£æ–‡ã€‘\\n{body}\\n\\n\"\n",
    "                else: \n",
    "                    prompt_context = f\"ã€é‚®ä»¶æ­£æ–‡ã€‘\\n{body}\\n\\nã€é™„ä»¶: {fn}ã€‘\\n{attach_text}\"\n",
    "\n",
    "                prompt = (\n",
    "                    f\"é‚®ä»¶ä¸»é¢˜: {subj}\\n\"\n",
    "                    f\"å‘ä»¶äºº: {sender_name} <{sender_email}>\\n\\n\"\n",
    "                    f\"{prompt_context}\"\n",
    "                )\n",
    "                \n",
    "                ans = glm(prompt)\n",
    "                parsed = parse_glm(ans)\n",
    "\n",
    "                if parsed:\n",
    "                    print(f\"    â†ª GLM parsed {len(parsed)} row(s) from {source_name}\")\n",
    "                    for item in parsed:\n",
    "                        row = {c: \"\" for c in COLS}\n",
    "                        row.update(item) \n",
    "                        row.update({\n",
    "                            \"åŸé‚®ä»¶å\": subj,\n",
    "                            \"å‘ä»¶äºº\": sender_email,\n",
    "                            \"å‘ä»¶æœºæ„\": sender_name \n",
    "                        })\n",
    "                        rows.append(row)\n",
    "                else:\n",
    "                    print(f\"    â†ª 0 rows parsed (or parsing failed) from {source_name}\")\n",
    "    \n",
    "    if actual_emails_processed_count == 0:\n",
    "        print(\"\\nğŸ‘€ No new emails were found and processed in this run.\")\n",
    "        save_current_run_datetime() \n",
    "        return\n",
    "\n",
    "    if not rows:\n",
    "        print(\"\\nğŸ‘€ Processed new emails, but no NAV data was captured.\")\n",
    "        save_current_run_datetime() \n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    if df.empty:\n",
    "        print(\"\\nğŸ‘€ No unique NAV data captured after processing and removing duplicates.\")\n",
    "        save_current_run_datetime() \n",
    "        return\n",
    "    \n",
    "    file_exists = pathlib.Path(XLSX).exists()\n",
    "    excel_writer_mode = \"a\" if file_exists else \"w\" \n",
    "    excel_if_sheet_exists = \"replace\" # Always replace if sheet exists, relevant for mode 'a'\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(XLSX, engine=\"openpyxl\", mode=excel_writer_mode, \n",
    "                            if_sheet_exists=excel_if_sheet_exists) as writer:\n",
    "            # If mode='w' or file didn't exist, it creates a new file.\n",
    "            # If mode='a' and sheet exists, it's replaced.\n",
    "            # If mode='a' and sheet doesn't exist, it's added.\n",
    "            df.to_excel(writer, index=False, sheet_name=SHEET, header=True)\n",
    "        print(f\"\\nâœ… {len(df)} unique rows written/updated â†’ {XLSX} (Sheet: {SHEET})\")\n",
    "    except Exception as e:\n",
    "        print(f\"    â€¼ï¸ Error writing to Excel '{XLSX}': {e}.\")\n",
    "        timestamp_fallback = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        fallback_xlsx = f\"{pathlib.Path(XLSX).stem}_fallback_{timestamp_fallback}{pathlib.Path(XLSX).suffix}\"\n",
    "        try:\n",
    "            df.to_excel(fallback_xlsx, index=False, sheet_name=SHEET)\n",
    "            print(f\"\\nâš ï¸ Data saved to fallback file: {fallback_xlsx}\")\n",
    "        except Exception as fe:\n",
    "            print(f\"    â€¼ï¸ Error writing to fallback Excel file '{fallback_xlsx}': {fe}.\")\n",
    "            print(f\"    â„¹ï¸ Raw data rows collected ({len(df)}):\")\n",
    "            # Limiting output for very large dataframes\n",
    "            # for record_idx, record in enumerate(df.to_dict('records')):\n",
    "            #     if record_idx < 10: # Print first 10 records\n",
    "            #         print(f\"      {record}\")\n",
    "            #     elif record_idx == 10:\n",
    "            #         print(f\"      ... (and {len(df)-10} more records)\")\n",
    "            #         break\n",
    "\n",
    "\n",
    "    save_current_run_datetime() # Save timestamp after all processing for this run is complete\n",
    "\n",
    "# â”€â”€â”€ run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nğŸ›‘ Script interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nğŸ’¥ An unexpected error occurred in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # This message now prints regardless of success, interrupt, or error in main()\n",
    "        print(\"\\nğŸ‘‹ Script execution cycle finished or was terminated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
